{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a self-attention Transformer model\n",
    "\n",
    "In this notebook, we will build the Transformer model for the classification task. The main architecture of the Transformer is derived from the paper: https://arxiv.org/pdf/1706.03762.pdf, but to be able to perform text classification we have to re-build the model a bit by applying the Max or Avg Pooling according to https://arxiv.org/pdf/1705.02364.pdf, where instead of using hidden representations we will us the last Transfomer block output.\n",
    "\n",
    "The Transformer is solely based on the self-attention mechanism, disposing recurrent units or convolution layers at all, thanks to which that architecture is superior in terms of the prediction quality and the training time. The Transformer allows for significantly more parallelization and keeps also the ability of discerning long-term dependencies. To increase the generalization performance of the model we will use the label smoothing method.\n",
    "\n",
    "The model is going to be trained on the clean_review column from the training dataset. In the end, the model will be evaluated on the test set to determine the generalization error.\n",
    "\n",
    "We will perform the hyperparameter fine-tuning and visualize model's learning curves to compare the model's performance while working on different set of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training the model\n",
    "\n",
    "Let's start with importing all indispensable libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_iterator import BatchIterator\n",
    "from early_stopping import EarlyStopping\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import device\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we will use the clean_review column from the training set as well as the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset. Use clean_review and label columns\n",
    "train_dataset = pd.read_csv('dataset/drugreview_feat_clean/train_feat_clean.csv', \n",
    "                            usecols=['clean_review', 'rating'])\n",
    "\n",
    "# Change columns order\n",
    "train_dataset['label'] = train_dataset.rating >= 5\n",
    "train_dataset = train_dataset[['clean_review', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>young suffering severe extreme neck pain resul...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>found work helping good nights sleep don&amp;#039;...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>given medication gastroenterologist office wor...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>recently laparoscopic hysterectomy know anesth...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>mirena year experienced effects effects watch ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_review  label\n",
       "2   young suffering severe extreme neck pain resul...   True\n",
       "5   found work helping good nights sleep don&#039;...   True\n",
       "9   given medication gastroenterologist office wor...  False\n",
       "12  recently laparoscopic hysterectomy know anesth...   True\n",
       "13  mirena year experienced effects effects watch ...  False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depict the first 5 rows of the training set\n",
    "train_dataset = train_dataset.dropna()\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine-tune the hyperparameters we will evaluate the model on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset. Use clean_review and label columns\n",
    "val_dataset = pd.read_csv('dataset/drugreview_feat_clean/val_feat_clean.csv',\n",
    "                          usecols=['clean_review', 'rating'])\n",
    "\n",
    "# Change columns order\n",
    "val_dataset['label'] = val_dataset.rating >= 5\n",
    "val_dataset = val_dataset[['clean_review', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>year old son took night went deep sea fishing ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>daughter epiduo grade junior year work wonders...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>i&amp;#039;ve implant months day got totally felt ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>wanted wait days post couldn&amp;#039;t results am...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>colonoscopy best prep far morning took prep pm...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_review  label\n",
       "0  year old son took night went deep sea fishing ...   True\n",
       "1  daughter epiduo grade junior year work wonders...   True\n",
       "2  i&#039;ve implant months day got totally felt ...   True\n",
       "3  wanted wait days post couldn&#039;t results am...   True\n",
       "4  colonoscopy best prep far morning took prep pm...   True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depict the first 5 rows of the validation set\n",
    "val_dataset = val_dataset.dropna()\n",
    "val_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the BatchIterator class to preprocess the text data and generate batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using as minimum count threashold: count = 3.00\n",
      "8674/21861 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 58\n",
      "Mapped words to indices\n",
      "Batches created\n"
     ]
    }
   ],
   "source": [
    "train_iterator = BatchIterator(train_dataset, batch_size=batch_size, vocab_created=False, vocab=None, target_col=None,\n",
    "                               word2index=None, sos_token='<SOS>', eos_token='<EOS>', unk_token='<UNK>',\n",
    "                               pad_token='<PAD>', min_word_count=3, max_vocab_size=None, max_seq_len=0.9,\n",
    "                               use_pretrained_vectors=False, glove_path='glove/', glove_name='glove.6B.100d.txt',\n",
    "                               weights_file_name='glove/weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using as minimum count threashold: count = 3.00\n",
      "4655/11853 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 57\n",
      "Mapped words to indices\n",
      "Batches created\n"
     ]
    }
   ],
   "source": [
    "val_iterator = BatchIterator(val_dataset, batch_size=batch_size, vocab_created=False, vocab=None, target_col=None,\n",
    "                             word2index=train_iterator.word2index, sos_token='<SOS>', eos_token='<EOS>',\n",
    "                             unk_token='<UNK>', pad_token='<PAD>', min_word_count=3, max_vocab_size=None,\n",
    "                             max_seq_len=0.9, use_pretrained_vectors=False, glove_path='glove/',\n",
    "                             glove_name='glove.6B.100d.txt', weights_file_name='glove/weights.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check out if the batches look correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_seq shape:  torch.Size([32, 4])\n",
      "target shape:  torch.Size([32])\n",
      "x_lengths shape:  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batches in train_iterator:\n",
    "    # Unpack the dictionary of batches\n",
    "    input_seq, target, x_lengths = batches['input_seq'], batches['target'], batches['x_lengths']\n",
    "    print('input_seq shape: ', input_seq.size())\n",
    "    print('target shape: ', target.size())\n",
    "    print('x_lengths shape: ', x_lengths.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the maximum sequence length\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "for batches in train_iterator:\n",
    "    x_lengths = batches['x_lengths']\n",
    "    if max(x_lengths) > max_len:\n",
    "        max_len = int(max(x_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 59\n"
     ]
    }
   ],
   "source": [
    "print('Maximum sequence length: {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start implementing the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Implementation of the Multi-Head-Attention.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    dmodel: int\n",
    "        Dimensionality of the input embedding vector.\n",
    "    heads: int\n",
    "        Number of the self-attention operations to conduct in parallel. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dmodel, heads):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert dmodel % heads == 0, 'Embedding dimension is not divisible by number of heads'\n",
    "            \n",
    "        self.dmodel = dmodel\n",
    "        self.heads = heads\n",
    "        # Split dmodel (embedd dimension) into 'heads' number of chunks\n",
    "        # each chunk of size key_dim will be passed to different attention head\n",
    "        self.key_dim = dmodel // heads\n",
    "        \n",
    "        # keys, queries and values will be computed at once for all heads\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(self.dmodel, self.dmodel, bias=False),\n",
    "            nn.Linear(self.dmodel, self.dmodel, bias=False),\n",
    "            nn.Linear(self.dmodel, self.dmodel, bias=False)])\n",
    "        \n",
    "        self.concat = nn.Linear(self.dmodel, self.dmodel, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Perform Multi-Head-Attention.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: torch.Tensor\n",
    "            Batch of inputs - position encoded word embeddings ((batch_size, seq_length, embedding_dim)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Multi-Head-Attention output of a shape (batch_size, seq_len, dmodel)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.batch_size = inputs.size(0)\n",
    "        \n",
    "        assert inputs.size(2) == self.dmodel, 'Input sizes mismatch, dmodel={}, while embedd={}'\\\n",
    "            .format(self.dmodel, inputs.size(2))\n",
    "\n",
    "        # Inputs shape (batch_size, seq_length, embedding_dim)        \n",
    "        # Map input batch allong embedd dimension to query, key and value vectors with\n",
    "        # a shape of (batch_size, heads, seq_len, key_dim (dmodel // heads)) \n",
    "        # where 'heads' dimension corresponds o different attention head\n",
    "        query, key, value = [linear(x).view(self.batch_size, -1, self.heads, self.key_dim).transpose(1, 2)\\\n",
    "                             for linear, x in zip(self.linear, (inputs, inputs, inputs))]\n",
    "        \n",
    "        # Calculate the score (batch_size, heads, seq_len, seq_len)\n",
    "        # for all heads at once\n",
    "        score = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.key_dim)\n",
    "        \n",
    "        # Apply softmax to scores (batch_size, heads, seq_len, seq_len) \n",
    "        soft_score = F.softmax(score, dim = -1)\n",
    "        \n",
    "        # Multiply softmaxed score and value vector\n",
    "        # value input shape (batch_size, heads, seq_len, key_dim)\n",
    "        # out shape (batch_size, seq_len, dmodel (key_dim * heads))\n",
    "        out = torch.matmul(soft_score, value).transpose(1, 2).contiguous()\\\n",
    "            .view(self.batch_size, -1, self.heads * self.key_dim)\n",
    "        \n",
    "        # Concatenate and linearly transform heads to the lower dimensional space\n",
    "        # out shape (batch_size, seq_len, dmodel)\n",
    "        out = self.concat(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Implementation of the positional encoding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    max_len: int\n",
    "        The maximum expected sequence length.\n",
    "    dmodel: int\n",
    "        Dimensionality of the input embedding vector.\n",
    "    dropout: float\n",
    "        Probability of an element of the tensor to be zeroed.\n",
    "    padding_idx: int\n",
    "        Index of the padding token in the vocabulary and word embedding.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_len, dmodel, dropout, padding_idx):\n",
    "        \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "                \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create pos_encoding, positions and dimensions matrices\n",
    "        # with a shape of (max_len, dmodel)\n",
    "        self.pos_encoding = torch.zeros(max_len, dmodel)\n",
    "        positions = torch.repeat_interleave(torch.arange(float(max_len)).unsqueeze(1), dmodel, dim=1)\n",
    "        dimensions = torch.arange(float(dmodel)).repeat(max_len, 1)\n",
    "                                  \n",
    "        # Calculate the encodings trigonometric function argument (max_len, dmodel)\n",
    "        trig_fn_arg = positions / (torch.pow(10000, 2 * dimensions / dmodel))\n",
    "               \n",
    "        # Encode positions using sin function for even dimensions and\n",
    "        # cos function for odd dimensions\n",
    "        self.pos_encoding[:, 0::2] = torch.sin(trig_fn_arg[:, 0::2])\n",
    "        self.pos_encoding[:, 1::2] = torch.cos(trig_fn_arg[:, 1::2])\n",
    "        \n",
    "        # Set the padding positional encoding to zero tensor\n",
    "        if padding_idx:\n",
    "            self.pos_encoding[padding_idx] = 0.0\n",
    "        \n",
    "        # Add batch dimension\n",
    "        self.pos_encoding = self.pos_encoding.unsqueeze(0)\n",
    "        \n",
    "        \n",
    "    def forward(self, embedd):\n",
    "        \"\"\"Apply positional encoding.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        embedd: torch.Tensor\n",
    "            Batch of word embeddings ((batch_size, seq_length, dmodel = embedding_dim))\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Sum of word embeddings and positional embeddings (batch_size, seq_length, dmodel)\n",
    "        \"\"\"\n",
    "        \n",
    "        # embedd shape (batch_size, seq_length, embedding_dim)\n",
    "        # pos_encoding shape (1, max_len, dmodel = embedd_dim)\n",
    "        embedd = embedd + self.pos_encoding[:, :embedd.size(1), :]\n",
    "        embedd = self.dropout(embedd)\n",
    "        \n",
    "        # embedd shape (batch_size, seq_length, embedding_dim)\n",
    "        return embedd  \n",
    "    \n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \"\"\"Implementation of label smoothing with the Kullback-Leibler divergence Loss.\n",
    "    \n",
    "    Example:\n",
    "    label_smoothing/(output_size-1) = 0.1\n",
    "    confidence = 1 - 0.1 = 0.9\n",
    "        \n",
    "    True labels      Smoothed one-hot labels\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |1|              [0.1000, 0.9000]\n",
    "        |1|    label     [0.1000, 0.9000]\n",
    "        |0|  smoothing   [0.9000, 0.1000]\n",
    "        |1|    ---->     [0.1000, 0.9000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |1|              [0.1000, 0.9000]\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_size: int\n",
    "         The number of classes.\n",
    "    label_smoothing: float, optional (default=0)\n",
    "        The smoothing parameter. Takes the value in range [0,1].\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_size, label_smoothing=0):\n",
    "\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.confidence = 1 - self.label_smoothing\n",
    "        \n",
    "        assert label_smoothing >= 0.0 and label_smoothing <= 1.0, \\\n",
    "        'Label smoothing parameter takes values in the range [0, 1]'\n",
    "\n",
    "        self.criterion = nn.KLDivLoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"Smooth the target labels and calculate the Kullback-Leibler divergence loss.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        pred: torch.Tensor\n",
    "            Batch of log-probabilities (batch_size, output_size)\n",
    "        target: torch.Tensor\n",
    "            Batch of target labels (batch_size, seq_length)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The Kullback-Leibler divergence Loss.\n",
    "    \n",
    "        \"\"\"\n",
    "        # Create a Tensor of targets probabilities of a shape that equals 'pred' dimensions, filled all\n",
    "        # with label_smoothing/(output_size-1) value that will correspond to the wrong label probability.\n",
    "        one_hot_probs = torch.full(size=pred.size(), fill_value=self.label_smoothing/(self.output_size - 1))\n",
    "        \n",
    "        # Fill the tensor at positions that correspond to the true label from the target vector (0/1)\n",
    "        # with the modified value of maximum probability (confidence).\n",
    "        one_hot_probs.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "            \n",
    "        # KLDivLoss takes inputs (pred) that contain log-probs and targets given as probs (one_hot_probs).\n",
    "        return self.criterion(pred, one_hot_probs)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Implementation of single Transformer block.\n",
    "    \n",
    "    Transformer block structure:\n",
    "    x --> Multi-Head --> Layer normalization --> Pos-Wise FFNN --> Layer normalization --> y\n",
    "      |   Attention   |                       |                 |\n",
    "      |_______________|                       |_________________|\n",
    "     residual connection                      residual connection\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dmodel: int\n",
    "        Dimensionality of the input embedding vector.\n",
    "    ffnn_hidden_size: int\n",
    "        Position-Wise-Feed-Forward Neural Network hidden size.\n",
    "    heads: int\n",
    "        Number of the self-attention operations to conduct in parallel.\n",
    "    dropout: float\n",
    "        Probability of an element of the tensor to be zeroed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dmodel, ffnn_hidden_size, heads, dropout):\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(dmodel, heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(dmodel)\n",
    "        self.layer_norm2 = nn.LayerNorm(dmodel)\n",
    "        \n",
    "        self.ffnn = nn.Sequential(\n",
    "                nn.Linear(dmodel, ffnn_hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(ffnn_hidden_size, dmodel))\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward propagate through the Transformer block.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: torch.Tensor\n",
    "            Batch of embeddings.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output of the Transformer block (batch_size, seq_length, dmodel)\n",
    "        \"\"\"\n",
    "        # Inputs shape (batch_size, seq_length, embedding_dim = dmodel)\n",
    "        output = inputs + self.attention(inputs)            \n",
    "        output = self.layer_norm1(output)            \n",
    "        output = output + self.ffnn(output)            \n",
    "        output = self.layer_norm2(output)\n",
    "\n",
    "        # Output shape (batch_size, seq_length, dmodel)\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Implementation of the Transformer model for classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size: int\n",
    "        The size of the vocabulary.\n",
    "    dmodel: int\n",
    "        Dimensionality of the embedding vector.\n",
    "    max_len: int\n",
    "        The maximum expected sequence length.\n",
    "    padding_idx: int, optional (default=0)\n",
    "        Index of the padding token in the vocabulary and word embedding.\n",
    "    n_layers: int, optional (default=4)\n",
    "        Number of the stacked Transformer blocks.    \n",
    "    ffnn_hidden_size: int, optonal (default=dmodel * 4)\n",
    "        Position-Wise-Feed-Forward Neural Network hidden size.\n",
    "    heads: int, optional (default=8)\n",
    "        Number of the self-attention operations to conduct in parallel.\n",
    "    pooling: str, optional (default='max')\n",
    "        Specify the type of pooling to use. Available options: 'max' or 'avg'.\n",
    "    dropout: float, optional (default=0.2)\n",
    "        Probability of an element of the tensor to be zeroed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, dmodel, output_size, max_len, padding_idx=0, n_layers=4,\n",
    "                 ffnn_hidden_size=None, heads=8, pooling='max', dropout=0.2):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        if not ffnn_hidden_size:\n",
    "            ffnn_hidden_size = dmodel * 4\n",
    "            \n",
    "        assert pooling == 'max' or pooling == 'avg', 'Improper pooling type was passed.'\n",
    "        \n",
    "        self.pooling = pooling\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, dmodel)\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(max_len, dmodel, dropout, padding_idx)\n",
    "        \n",
    "        self.tnf_blocks = nn.ModuleList()\n",
    "        \n",
    "        for n in range(n_layers):\n",
    "            self.tnf_blocks.append(\n",
    "                TransformerBlock(dmodel, ffnn_hidden_size, heads, dropout))\n",
    "            \n",
    "        self.tnf_blocks = nn.Sequential(*self.tnf_blocks)\n",
    "            \n",
    "        self.linear = nn.Linear(dmodel, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        \"\"\"Forward propagate through the Transformer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: torch.Tensor\n",
    "            Batch of input sequences.\n",
    "        input_lengths: torch.LongTensor\n",
    "            Batch containing sequences lengths.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Logarithm of softmaxed class tensor.\n",
    "        \"\"\"\n",
    "        self.batch_size = inputs.size(0)\n",
    "        \n",
    "        # Input dimensions (batch_size, seq_length, dmodel)\n",
    "        output = self.embedding(inputs)\n",
    "        output = self.pos_encoding(output)\n",
    "        output = self.tnf_blocks(output)\n",
    "        # Output dimensions (batch_size, seq_length, dmodel)\n",
    "        \n",
    "        if self.pooling == 'max':\n",
    "            # Permute to the shape (batch_size, dmodel, seq_length)\n",
    "            # Apply max-pooling, output dimensions (batch_size, dmodel)\n",
    "            output = F.adaptive_max_pool1d(output.permute(0,2,1), (1,)).view(self.batch_size,-1)\n",
    "        else:\n",
    "            # Sum along the batch axis and divide by the corresponding lengths (FloatTensor)\n",
    "            # Output shape: (batch_size, dmodel)\n",
    "            output = torch.sum(output, dim=1) / input_lengths.view(-1,1).type(torch.FloatTensor) \n",
    "            \n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        \n",
    "    def add_loss_fn(self, loss_fn):\n",
    "        \"\"\"Add loss function to the model.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "\n",
    "    def add_optimizer(self, optimizer):\n",
    "        \"\"\"Add optimizer to the model.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        \n",
    "    def add_device(self, device=torch.device('cpu')):\n",
    "        \"\"\"Specify the device.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "    \n",
    "    \n",
    "    def train_model(self, train_iterator):\n",
    "        \"\"\"Perform single training epoch.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_iterator: BatchIterator\n",
    "            BatchIterator class object containing training batches.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        train_losses: list\n",
    "            List of the training average batch losses.\n",
    "        avg_loss: float\n",
    "            Average loss on the entire training set.\n",
    "        accuracy: float\n",
    "            Models accuracy on the entire training set.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        \n",
    "        train_losses = []\n",
    "        losses = []\n",
    "        losses_list = []\n",
    "        num_seq = 0\n",
    "        batch_correct = 0\n",
    "            \n",
    "        for i, batches in tqdm_notebook(enumerate(train_iterator, 1), total=len(train_iterator), desc='Training'):\n",
    "            input_seq, target, x_lengths = batches['input_seq'], batches['target'], batches['x_lengths']\n",
    "            \n",
    "            input_seq.to(self.device)\n",
    "            target.to(self.device)\n",
    "            x_lengths.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            pred = self.forward(input_seq, x_lengths)\n",
    "            loss = self.loss_fn(pred, target)\n",
    "            loss.backward()\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            losses_list.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            pred = torch.argmax(pred, 1)\n",
    "\n",
    "            if self.device.type == 'cpu':\n",
    "                batch_correct += (pred.cpu() == target.cpu()).sum().item()\n",
    "\n",
    "            else:\n",
    "                batch_correct += (pred == target).sum().item()\n",
    "\n",
    "            num_seq += len(input_seq)     \n",
    "    \n",
    "            if i % 100 == 0:\n",
    "                avg_train_loss = np.mean(losses)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                \n",
    "                accuracy = batch_correct / num_seq\n",
    "                \n",
    "                print('Iteration: {}. Average training loss: {:.4f}. Accuracy: {:.3f}'\\\n",
    "                      .format(i, avg_train_loss, accuracy))\n",
    "                \n",
    "                losses = []\n",
    "                \n",
    "            avg_loss = np.mean(losses_list)\n",
    "            accuracy = batch_correct / num_seq\n",
    "                              \n",
    "        return train_losses, avg_loss, accuracy\n",
    "    \n",
    "    \n",
    "    def evaluate_model(self, eval_iterator, conf_mtx=False):\n",
    "        \"\"\"Perform the one evaluation epoch.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        eval_iterator: BatchIterator\n",
    "            BatchIterator class object containing evaluation batches.\n",
    "        conf_mtx: boolean, optional (default=False)\n",
    "            Whether to print the confusion matrix at each epoch.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        eval_losses: list\n",
    "            List of the evaluation average batch losses.\n",
    "        avg_loss: float\n",
    "            Average loss on the entire evaluation set.\n",
    "        accuracy: float\n",
    "            Models accuracy on the entire evaluation set.\n",
    "        conf_matrix: list\n",
    "            Confusion matrix.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        eval_losses = []\n",
    "        losses = []\n",
    "        losses_list = []\n",
    "        num_seq = 0\n",
    "        batch_correct = 0\n",
    "        pred_total = torch.LongTensor()\n",
    "        target_total = torch.LongTensor()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batches in tqdm_notebook(enumerate(eval_iterator, 1), total=len(eval_iterator), desc='Evaluation'):\n",
    "                input_seq, target, x_lengths = batches['input_seq'], batches['target'], batches['x_lengths']\n",
    "                \n",
    "                input_seq.to(self.device)\n",
    "                target.to(self.device)\n",
    "                x_lengths.to(self.device)\n",
    "\n",
    "                pred = self.forward(input_seq, x_lengths)\n",
    "                loss = self.loss_fn(pred, target)\n",
    "                losses.append(loss.data.cpu().numpy())\n",
    "                losses_list.append(loss.data.cpu().numpy())\n",
    "                \n",
    "                pred = torch.argmax(pred, 1)\n",
    "                                \n",
    "                if self.device.type == 'cpu':\n",
    "                    batch_correct += (pred.cpu() == target.cpu()).sum().item()\n",
    "                    \n",
    "                else:\n",
    "                    batch_correct += (pred == target).sum().item()\n",
    "                    \n",
    "                num_seq += len(input_seq)     \n",
    "                \n",
    "                pred_total = torch.cat([pred_total, pred], dim=0)\n",
    "                target_total = torch.cat([target_total, target], dim=0)\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    avg_batch_eval_loss = np.mean(losses)\n",
    "                    eval_losses.append(avg_batch_eval_loss)\n",
    "                    \n",
    "                    accuracy = batch_correct / num_seq\n",
    "                    \n",
    "                    print('Iteration: {}. Average evaluation loss: {:.4f}. Accuracy: {:.2f}'\\\n",
    "                          .format(i, avg_batch_eval_loss, accuracy))\n",
    "\n",
    "                    losses = []\n",
    "                    \n",
    "            avg_loss_list = []\n",
    "                    \n",
    "            avg_loss = np.mean(losses_list)\n",
    "            accuracy = batch_correct / num_seq\n",
    "            \n",
    "            conf_matrix = confusion_matrix(target_total.view(-1), pred_total.view(-1))\n",
    "        \n",
    "        if conf_mtx:\n",
    "            print('\\tConfusion matrix: ', conf_matrix)\n",
    "            \n",
    "        return eval_losses, avg_loss, accuracy, conf_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start epoch [1/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3d3b47afa84ad4917dbc7ea164c1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1547. Accuracy: 0.728\n",
      "Iteration: 200. Average training loss: 0.1432. Accuracy: 0.739\n",
      "Iteration: 300. Average training loss: 0.1455. Accuracy: 0.742\n",
      "Iteration: 400. Average training loss: 0.1451. Accuracy: 0.743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaacfe5c04d34cda8e14e6a563ffb078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1322. Accuracy: 0.76\n",
      "\n",
      "Epoch [1/30]: Train accuracy: 0.743. Train loss: 0.1469. Evaluation accuracy: 0.757. Evaluation loss: 0.1339\n",
      "\n",
      "Start epoch [2/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6d0835372b4b3887421cb1502a71bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1413. Accuracy: 0.748\n",
      "Iteration: 200. Average training loss: 0.1370. Accuracy: 0.751\n",
      "Iteration: 300. Average training loss: 0.1373. Accuracy: 0.750\n",
      "Iteration: 400. Average training loss: 0.1343. Accuracy: 0.749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9b159e40524b4f8713b846b1d6e06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1196. Accuracy: 0.79\n",
      "\n",
      "Epoch [2/30]: Train accuracy: 0.750. Train loss: 0.1368. Evaluation accuracy: 0.781. Evaluation loss: 0.1212\n",
      "\n",
      "Start epoch [3/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01756ed1177a475985d353596bed76d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1312. Accuracy: 0.748\n",
      "Iteration: 200. Average training loss: 0.1259. Accuracy: 0.754\n",
      "Iteration: 300. Average training loss: 0.1253. Accuracy: 0.760\n",
      "Iteration: 400. Average training loss: 0.1254. Accuracy: 0.761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a6a7a5b81a481094a75b6693e5467c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1140. Accuracy: 0.79\n",
      "\n",
      "Epoch [3/30]: Train accuracy: 0.762. Train loss: 0.1263. Evaluation accuracy: 0.786. Evaluation loss: 0.1151\n",
      "\n",
      "Start epoch [4/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2bec744b1b46ffba737d264423940d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1239. Accuracy: 0.765\n",
      "Iteration: 200. Average training loss: 0.1226. Accuracy: 0.770\n",
      "Iteration: 300. Average training loss: 0.1185. Accuracy: 0.773\n",
      "Iteration: 400. Average training loss: 0.1194. Accuracy: 0.773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcd01b30e5949e785241fedf697cc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1082. Accuracy: 0.80\n",
      "\n",
      "Epoch [4/30]: Train accuracy: 0.775. Train loss: 0.1203. Evaluation accuracy: 0.792. Evaluation loss: 0.1094\n",
      "\n",
      "Start epoch [5/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cf52dbdc594587867db4d6db41f6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1188. Accuracy: 0.784\n",
      "Iteration: 200. Average training loss: 0.1170. Accuracy: 0.787\n",
      "Iteration: 300. Average training loss: 0.1160. Accuracy: 0.786\n",
      "Iteration: 400. Average training loss: 0.1133. Accuracy: 0.786\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cfaa707de142d9a64d3206a35f8285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1051. Accuracy: 0.80\n",
      "\n",
      "Epoch [5/30]: Train accuracy: 0.787. Train loss: 0.1153. Evaluation accuracy: 0.802. Evaluation loss: 0.1063\n",
      "\n",
      "Start epoch [6/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fece0ca77a4b4b9487a9b15559cc1d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1089. Accuracy: 0.795\n",
      "Iteration: 200. Average training loss: 0.1097. Accuracy: 0.796\n",
      "Iteration: 300. Average training loss: 0.1059. Accuracy: 0.800\n",
      "Iteration: 400. Average training loss: 0.1097. Accuracy: 0.798\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c259db618a904bfbb2ac0e302664ef2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1023. Accuracy: 0.81\n",
      "\n",
      "Epoch [6/30]: Train accuracy: 0.799. Train loss: 0.1081. Evaluation accuracy: 0.808. Evaluation loss: 0.1034\n",
      "\n",
      "Start epoch [7/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e16e8b1490a4c3b80a8a4eb92322b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1100. Accuracy: 0.794\n",
      "Iteration: 200. Average training loss: 0.1058. Accuracy: 0.800\n",
      "Iteration: 300. Average training loss: 0.1029. Accuracy: 0.801\n",
      "Iteration: 400. Average training loss: 0.1033. Accuracy: 0.804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79915d22dd874d59a007010582d96950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1001. Accuracy: 0.82\n",
      "\n",
      "Epoch [7/30]: Train accuracy: 0.805. Train loss: 0.1051. Evaluation accuracy: 0.818. Evaluation loss: 0.1014\n",
      "\n",
      "Start epoch [8/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc77727ba55f45a6abaedb0175183c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1026. Accuracy: 0.816\n",
      "Iteration: 200. Average training loss: 0.1052. Accuracy: 0.810\n",
      "Iteration: 300. Average training loss: 0.1030. Accuracy: 0.811\n",
      "Iteration: 400. Average training loss: 0.1002. Accuracy: 0.812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47bcd6a382a647d9a1fb0bd4a222be06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0985. Accuracy: 0.82\n",
      "\n",
      "Epoch [8/30]: Train accuracy: 0.813. Train loss: 0.1024. Evaluation accuracy: 0.817. Evaluation loss: 0.1000\n",
      "\n",
      "Start epoch [9/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7095cf43b1524e9ea2e1d2de3c130609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0984. Accuracy: 0.826\n",
      "Iteration: 200. Average training loss: 0.0991. Accuracy: 0.823\n",
      "Iteration: 300. Average training loss: 0.0987. Accuracy: 0.825\n",
      "Iteration: 400. Average training loss: 0.0970. Accuracy: 0.827\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1258c1a410d43748d6be28be67f9769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0988. Accuracy: 0.82\n",
      "\n",
      "Epoch [9/30]: Train accuracy: 0.827. Train loss: 0.0980. Evaluation accuracy: 0.812. Evaluation loss: 0.1006\n",
      "\n",
      "Start epoch [10/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef2909b0c0a47ddb1927033138bf783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0985. Accuracy: 0.825\n",
      "Iteration: 200. Average training loss: 0.0963. Accuracy: 0.825\n",
      "Iteration: 300. Average training loss: 0.0952. Accuracy: 0.826\n",
      "Iteration: 400. Average training loss: 0.0936. Accuracy: 0.827\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d5b57d6fbc4bfa996a73b070547dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0970. Accuracy: 0.83\n",
      "\n",
      "Epoch [10/30]: Train accuracy: 0.827. Train loss: 0.0956. Evaluation accuracy: 0.821. Evaluation loss: 0.0985\n",
      "\n",
      "Start epoch [11/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2661dda38e0e4214b0a544bf08472f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0938. Accuracy: 0.828\n",
      "Iteration: 200. Average training loss: 0.0951. Accuracy: 0.828\n",
      "Iteration: 300. Average training loss: 0.0938. Accuracy: 0.829\n",
      "Iteration: 400. Average training loss: 0.0936. Accuracy: 0.830\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0faf537a24b403d871bcff139b15d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0969. Accuracy: 0.83\n",
      "\n",
      "Epoch [11/30]: Train accuracy: 0.832. Train loss: 0.0936. Evaluation accuracy: 0.828. Evaluation loss: 0.0985\n",
      "\n",
      "Start epoch [12/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d731e222e049c1bcbe5f9c4c6ab9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0936. Accuracy: 0.833\n",
      "Iteration: 200. Average training loss: 0.0913. Accuracy: 0.833\n",
      "Iteration: 300. Average training loss: 0.0915. Accuracy: 0.835\n",
      "Iteration: 400. Average training loss: 0.0904. Accuracy: 0.835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7261016f2b94cdf83dc777a03d83e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1003. Accuracy: 0.82\n",
      "\n",
      "Epoch [12/30]: Train accuracy: 0.835. Train loss: 0.0914. Evaluation accuracy: 0.817. Evaluation loss: 0.1023\n",
      "\n",
      "Start epoch [13/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3ac079a2c04cad957337d865e52ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0894. Accuracy: 0.845\n",
      "Iteration: 200. Average training loss: 0.0892. Accuracy: 0.842\n",
      "Iteration: 300. Average training loss: 0.0896. Accuracy: 0.841\n",
      "Iteration: 400. Average training loss: 0.0916. Accuracy: 0.840\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d15f3deeadd4508a466f26369fe2f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1000. Accuracy: 0.83\n",
      "\n",
      "Epoch [13/30]: Train accuracy: 0.841. Train loss: 0.0895. Evaluation accuracy: 0.824. Evaluation loss: 0.1018\n",
      "\n",
      "Start epoch [14/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004d563e627c42bb997f60490a827b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0881. Accuracy: 0.843\n",
      "Iteration: 200. Average training loss: 0.0880. Accuracy: 0.842\n",
      "Iteration: 300. Average training loss: 0.0878. Accuracy: 0.844\n",
      "Iteration: 400. Average training loss: 0.0844. Accuracy: 0.846\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d3d8f8995e45e7b72e1462b6452557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1015. Accuracy: 0.83\n",
      "\n",
      "Epoch [14/30]: Train accuracy: 0.846. Train loss: 0.0868. Evaluation accuracy: 0.820. Evaluation loss: 0.1038\n",
      "\n",
      "Training stoped by EarlyStopping\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "vocab_size = len(train_iterator.word2index)\n",
    "dmodel = 64\n",
    "output_size = 2\n",
    "padding_idx = train_iterator.word2index['<PAD>']\n",
    "n_layers = 4\n",
    "ffnn_hidden_size = dmodel * 2\n",
    "heads = 8\n",
    "pooling = 'max'\n",
    "dropout = 0.5\n",
    "label_smoothing = 0.1\n",
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "\n",
    "# Check whether system supports CUDA\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "model = Transformer(vocab_size, dmodel, output_size, max_len, padding_idx, n_layers,\\\n",
    "                    ffnn_hidden_size, heads, pooling, dropout)\n",
    "\n",
    "# Move the model to GPU if possible\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "    \n",
    "# Add loss function    \n",
    "if label_smoothing:\n",
    "    loss_fn = LabelSmoothingLoss(output_size, label_smoothing)\n",
    "else:\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    \n",
    "model.add_loss_fn(loss_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.add_optimizer(optimizer)\n",
    "\n",
    "device = torch.device('cuda' if CUDA else 'cpu')\n",
    "\n",
    "model.add_device(device)\n",
    "\n",
    "# Create the parameters dictionary and instantiate the tensorboardX SummaryWriter\n",
    "params = {'batch_size': batch_size,\n",
    "          'dmodel': dmodel,\n",
    "          'n_layers': n_layers,\n",
    "          'ffnn_hidden_size': ffnn_hidden_size,\n",
    "          'heads': heads,\n",
    "          'pooling': pooling,\n",
    "          'dropout': dropout,\n",
    "          'label_smoothing': label_smoothing,\n",
    "          'learning_rate': learning_rate}\n",
    "\n",
    "train_writer = SummaryWriter(comment=f' Training, batch_size={batch_size}, dmodel={dmodel}, n_layers={n_layers},\\\n",
    "ffnn_hidden_size={ffnn_hidden_size}, heads={heads}, pooling={pooling}, dropout={dropout}, \\\n",
    "label_smoothing={label_smoothing}, learning_rate={learning_rate}'.format(**params))\n",
    "\n",
    "val_writer = SummaryWriter(comment=f' Validation, batch_size={batch_size}, dmodel={dmodel}, n_layers={n_layers},\\\n",
    "ffnn_hidden_size={ffnn_hidden_size}, heads={heads}, pooling={pooling}, dropout={dropout}, \\\n",
    "label_smoothing={label_smoothing}, learning_rate={learning_rate}'.format(**params))\n",
    "\n",
    "# Instantiate the EarlyStopping\n",
    "early_stop = EarlyStopping(wait_epochs=3)\n",
    "\n",
    "train_losses_list, train_avg_loss_list, train_accuracy_list = [], [], []\n",
    "eval_avg_loss_list, eval_accuracy_list, conf_matrix_list = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    try:\n",
    "        print('\\nStart epoch [{}/{}]'.format(epoch+1, epochs))\n",
    "\n",
    "        train_losses, train_avg_loss, train_accuracy = model.train_model(train_iterator)\n",
    "\n",
    "        train_losses_list.append(train_losses)\n",
    "        train_avg_loss_list.append(train_avg_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "        _, eval_avg_loss, eval_accuracy, conf_matrix = model.evaluate_model(val_iterator)\n",
    "\n",
    "        eval_avg_loss_list.append(eval_avg_loss)\n",
    "        eval_accuracy_list.append(eval_accuracy)\n",
    "        conf_matrix_list.append(conf_matrix)\n",
    "\n",
    "        print('\\nEpoch [{}/{}]: Train accuracy: {:.3f}. Train loss: {:.4f}. Evaluation accuracy: {:.3f}. Evaluation loss: {:.4f}'\\\n",
    "              .format(epoch+1, epochs, train_accuracy, train_avg_loss, eval_accuracy, eval_avg_loss))\n",
    "\n",
    "        train_writer.add_scalar('Training loss', train_avg_loss, epoch)\n",
    "        val_writer.add_scalar('Validation loss', eval_avg_loss, epoch)\n",
    "\n",
    "        if early_stop.stop(eval_avg_loss, model, delta=0.003):\n",
    "            break\n",
    "\n",
    "    finally:\n",
    "        train_writer.close()\n",
    "        val_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(conf_matrix, fmt='d', annot=True, linewidths=1, square=True)\n",
    "ax.set_xlabel('Predictions', size=12)\n",
    "ax.set_ylabel('True labels', size=12) \n",
    "ax.set_title('Confusion Matrix', size=12); \n",
    "ax.xaxis.set_ticklabels(['True', 'False'])\n",
    "ax.yaxis.set_ticklabels(['True', 'False'])\n",
    "ax.set_ylim(2,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset. Use clean_review and label columns\n",
    "test_dataset = pd.read_csv('dataset/drugreview_feat_clean/test_feat_clean.csv',\n",
    "                          usecols=['clean_review', 'rating'])\n",
    "\n",
    "# Change columns order\n",
    "test_dataset['label'] = test_dataset.rating >= 5\n",
    "test_dataset = test_dataset[['clean_review', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>given sample doctor mg hours lower abdominal g...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>given medication post hysteroscopy suffered se...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>loperamide helpful diarrhea fewer caplets help...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>use claritin d seasonal allergies started taki...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>worked immediate effects noticeable long term</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_review  label\n",
       "2   given sample doctor mg hours lower abdominal g...  False\n",
       "3   given medication post hysteroscopy suffered se...   True\n",
       "4   loperamide helpful diarrhea fewer caplets help...   True\n",
       "10  use claritin d seasonal allergies started taki...   True\n",
       "15      worked immediate effects noticeable long term   True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = test_dataset.dropna()\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using as minimum count threashold: count = 3.00\n",
      "3069/8377 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 54\n",
      "Mapped words to indices\n",
      "Batches created\n"
     ]
    }
   ],
   "source": [
    "test_iterator = BatchIterator(test_dataset, batch_size=256, vocab_created=False, vocab=None, target_col=None,\n",
    "                              word2index=train_iterator.word2index, sos_token='<SOS>', eos_token='<EOS>',\n",
    "                              unk_token='<UNK>', pad_token='<PAD>', min_word_count=3, max_vocab_size=None,\n",
    "                              max_seq_len=0.9, use_pretrained_vectors=False, glove_path='glove/',\n",
    "                              glove_name='glove.6B.100d.txt', weights_file_name='glove/weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b656f1adc09b486aa684d2638f090972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, test_avg_loss, test_accuracy, test_conf_matrix = model.evaluate_model(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.817. Test error: 0.105\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: {:.3f}. Test error: {:.3f}'.format(test_accuracy, test_avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(test_conf_matrix, fmt='d', annot=True, linewidths=1, square=True)\n",
    "ax.set_xlabel('Predictions', size=12)\n",
    "ax.set_ylabel('True labels', size=12) \n",
    "ax.set_title('Confusion Matrix', size=12); \n",
    "ax.xaxis.set_ticklabels(['True', 'False'])\n",
    "ax.yaxis.set_ticklabels(['True', 'False'])\n",
    "ax.set_ylim(2,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

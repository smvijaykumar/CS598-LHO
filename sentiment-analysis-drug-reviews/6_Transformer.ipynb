{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a self-attention Transformer model\n",
    "\n",
    "In this notebook, we will build the Transformer model for the classification task. The main architecture of the Transformer is derived from the paper: https://arxiv.org/pdf/1706.03762.pdf, but to be able to perform text classification we have to re-build the model a bit by applying the Max or Avg Pooling according to https://arxiv.org/pdf/1705.02364.pdf, where instead of using hidden representations we will us the last Transfomer block output.\n",
    "\n",
    "The Transformer is solely based on the self-attention mechanism, disposing recurrent units or convolution layers at all, thanks to which that architecture is superior in terms of the prediction quality and the training time. The Transformer allows for significantly more parallelization and keeps also the ability of discerning long-term dependencies. To increase the generalization performance of the model we will use the label smoothing method.\n",
    "\n",
    "The model is going to be trained on the clean_review column from the training dataset. In the end, the model will be evaluated on the test set to determine the generalization error.\n",
    "\n",
    "We will perform the hyperparameter fine-tuning and visualize model's learning curves to compare the model's performance while working on different set of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-attention\n",
    "\n",
    "Attention is a function that maps the lineary transformed input sequence represented as three vectors - the query, the key and the value to the output vector. The output is calculated as a weighted sum of the values, where the weights are computed using compatibility function of queries and keys.\n",
    "\n",
    "The first step is to create the three aforementioned vectors by multiplying the embedding vector of each word (including positional encoding or embedding) by three weight matrices that are learned during training. The second dimension of these matrices can be of an arbitrary shape, this enables us to use query, key and value vectors with different dimensionality than the input embedding vector.\n",
    "\n",
    "<p><center>\n",
    "$q_{i} = W_{q} \\cdot x_{i} \\quad k_{i} = W_{k} \\cdot x_{i} \\quad v_{i} = W_{v} \\cdot x_{i}$\n",
    "</center></p>\n",
    "Where $x_{i}$ is the embedding vector and $W_{q}, W_{k}, W_{v}$ are the queries, keys and values weight matrices respectively.\n",
    "\n",
    "Next the score is obtained by using the alignment function (dot product). The score is calculated between a query vector that corresponds to the word for which we would like to compute the output and all the key vectors. To prevent softmax from taking as an input to high values that might cause the gradient vanishing problem we have to scale the dot product by dividing it by the square root of the lenght of the query/key vector. \n",
    "<p><center>\n",
    "$score = \\frac{q_{i}^T \\cdot\\: k_{j}}{\\sqrt{k_{dim}}}$\n",
    "</center></p>\n",
    "Where $k_{dim}$ is the key (query) vector dimension.\n",
    "\n",
    "Then all the scores are run through the softmax layer that makes that all of them are in the range from 0 through 1 and ensures that they add up to 1. Softmaxed score at given position is then multiply by the corresponding value vector. Next all the resulted vectors are summed up to produce the context vector that contains the information about how much attention to pay to each of the input words to predict the output at given position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"assets/Dot-product-attention.png\" width=\"1000\"/>\n",
    "</div>\n",
    "<p><center>\n",
    "Schema of Scaled Dot-Product Self-Attention for the word \"this\". \n",
    "</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "\n",
    "Multi-Head Attention is a mechanism introduced by https://arxiv.org/pdf/1706.03762.pdf that takes into consideration the fact that different words can have a different meaning to their adjacent words, which improves the model's ability to focus on a different part of an input sequence. Instead of performing single attention that sums all information together, which may cause that two similar sentences will have the same meaning for the model (what should not happen), we conduct self-attention operation *h* times in parallel, while each head (single attention) has its own different, randomly initialized weight matrices $W_{q}, W_{k}, W_{v}$. The resulted context vectors of each attention head are concatenated and multiplied by the additional weight matrix during linear transformation to the lower dimensional space.\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"assets/multi-head-attention.png\" width=\"220\"/>\n",
    "</div>\n",
    "<p><center>\n",
    "    Multi-Head Attention with <i>h</i> heads [https://arxiv.org/pdf/1706.03762.pdf]\n",
    "</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding\n",
    "\n",
    "The model's layers presented up to this time don't take into account the order of the words, they don't treat inputs as a sequence, but rather as a set, thus if we shuffle words in an input sequence we will obtain the same classification. To introduce into the model the possibility of including the order of words in its predictions, it is necessary to add the positional encoding or positional embedding layer to the architecture.\n",
    "\n",
    "In this implementation we will use the positional encoding that has the edge over the embedding, namely, it uses a function that maps words positions to vectors, thus it can be used also to encode words positions in sequences that are longer than those seen during training thanks to the extrapolation possibility.\n",
    "\n",
    "The sine and cosine functions can be used to encode words positions (use *sin* for even dimensions and *cos* for odd dimensions):\n",
    "<p><center>\n",
    "$PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$ <br>\n",
    "$PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$\n",
    "</center></p>\n",
    "\n",
    "Where *pos* is the position, *i* is the dimension and $d_{model}$ is the dimensionality of the positional encoding vector that equals word embedding vector shape.\n",
    "\n",
    "After creating positional encoding vector, it is summed together with the word embedding vector and passed to the first self-attention layer. In addition, we can apply dropout to the sum of these vectors before passing them forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label smoothing\n",
    "\n",
    "Label smoothing is a method that improves the generalization performance of the model by computing cross entropy with a weighted compound of targets with the uniform distribution, instead of using the â€œhard\" targets from the dataset.\n",
    "\n",
    "The soft targets are computed according to the following formula:<br>\n",
    "<p><center>\n",
    "$y_{k}^{LS} = \\begin{cases} 1 - \\alpha & correct\\:labels\\\\\\alpha / (k-1) & wrong\\:labels\\end{cases}$\n",
    "</center></p>\n",
    "    \n",
    "Where $\\alpha$ is the smoothing parameter and *k* corresponds to the number of classes.\n",
    "\n",
    "Then the target's vector is one-hot encoded using soft labels.\n",
    "\n",
    "    label_smoothing/(output_size-1) = 0.1\n",
    "    confidence = 1 - 0.1 = 0.9\n",
    "        \n",
    "    True labels      Smoothed one-hot labels\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |1|              [0.1000, 0.9000]\n",
    "        |1|    label     [0.1000, 0.9000]\n",
    "        |0|  smoothing   [0.9000, 0.1000]\n",
    "        |1|    ---->     [0.1000, 0.9000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |1|              [0.1000, 0.9000]\n",
    "        \n",
    "The label smoothing encourages the model to decrease its confidence while predicting the correct labels because it uses the modified labels for correct examples, that are smaller than one (1 - $\\alpha$), thus we will penalize the model from predicting too confidently (when confidence starting to rise above 1 - $\\alpha$ threshold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer block\n",
    "\n",
    "The Transformer architecture is composed of several identical layers that can be repeated, we call the set of these layers a block. Each block consists of the following layers: Multi-Head-Attention, layer normalization, position-wise fully connected feed-forward network and as the last one, another layer normalization. Around Multi-Head-Attention and Position-wise FFN the residual connections are used. The layer normalization is applied over the embedding dimension only. \n",
    " \n",
    "<br>\n",
    "<div>\n",
    "<img src=\"assets/transformer-block.png\" width=\"280\"/>\n",
    "</div>\n",
    "<p><center>\n",
    "    Single Transformer block\n",
    "</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer for classification - architecture\n",
    "\n",
    "The original model presented in the \"Attention Is All You Need\" has an encoder-decoder structure, which is used for seq2seq transformations. To prepare the Transformer for classification purposes we will take the similar approach we took when building the biGRU model, namely, we will apply the maximum or average pooling over the output of last Transformer block, instead of over hidden states. Then the pooled representation will be mapped to the softmaxed class tensor.\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"assets/Transformer-classifier.png\" width=\"290\"/>\n",
    "</div>\n",
    "<p><center>\n",
    "    Schema of the Transformer model for classification tasks.\n",
    "</center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training the model\n",
    "\n",
    "Let's start with importing all indispensable libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_iterator import BatchIterator\n",
    "from early_stopping import EarlyStopping\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import device\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we will use the clean_review column from the training set as well as the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset. Use clean_review and label columns\n",
    "train_dataset = pd.read_csv('dataset/drugreview_feat_clean/train_feat_clean.csv', \n",
    "                            usecols=['clean_review', 'rating'])\n",
    "\n",
    "# Change columns order\n",
    "train_dataset['label'] = train_dataset.rating >= 5\n",
    "train_dataset = train_dataset[['clean_review', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>young suffering severe extreme neck pain resul...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>found work helping good nights sleep don&amp;#039;...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>given medication gastroenterologist office wor...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>recently laparoscopic hysterectomy know anesth...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>mirena year experienced effects effects watch ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_review  label\n",
       "2   young suffering severe extreme neck pain resul...   True\n",
       "5   found work helping good nights sleep don&#039;...   True\n",
       "9   given medication gastroenterologist office wor...  False\n",
       "12  recently laparoscopic hysterectomy know anesth...   True\n",
       "13  mirena year experienced effects effects watch ...  False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depict the first 5 rows of the training set\n",
    "train_dataset = train_dataset.dropna()\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine-tune the hyperparameters we will evaluate the model on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset. Use clean_review and label columns\n",
    "val_dataset = pd.read_csv('dataset/drugreview_feat_clean/val_feat_clean.csv',\n",
    "                          usecols=['clean_review', 'rating'])\n",
    "\n",
    "# Change columns order\n",
    "val_dataset['label'] = val_dataset.rating >= 5\n",
    "val_dataset = val_dataset[['clean_review', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>year old son took night went deep sea fishing ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>daughter epiduo grade junior year work wonders...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>i&amp;#039;ve implant months day got totally felt ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>wanted wait days post couldn&amp;#039;t results am...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>colonoscopy best prep far morning took prep pm...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_review  label\n",
       "0  year old son took night went deep sea fishing ...   True\n",
       "1  daughter epiduo grade junior year work wonders...   True\n",
       "2  i&#039;ve implant months day got totally felt ...   True\n",
       "3  wanted wait days post couldn&#039;t results am...   True\n",
       "4  colonoscopy best prep far morning took prep pm...   True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depict the first 5 rows of the validation set\n",
    "val_dataset = val_dataset.dropna()\n",
    "val_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the BatchIterator class to preprocess the text data and generate batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using as minimum count threashold: count = 3.00\n",
      "8674/21861 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 58\n",
      "Mapped words to indices\n",
      "Batches created\n"
     ]
    }
   ],
   "source": [
    "train_iterator = BatchIterator(train_dataset, batch_size=batch_size, vocab_created=False, vocab=None, target_col=None,\n",
    "                               word2index=None, sos_token='<SOS>', eos_token='<EOS>', unk_token='<UNK>',\n",
    "                               pad_token='<PAD>', min_word_count=3, max_vocab_size=None, max_seq_len=0.9,\n",
    "                               use_pretrained_vectors=False, glove_path='glove/', glove_name='glove.6B.100d.txt',\n",
    "                               weights_file_name='glove/weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using as minimum count threashold: count = 3.00\n",
      "4655/11853 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 57\n",
      "Mapped words to indices\n",
      "Batches created\n"
     ]
    }
   ],
   "source": [
    "val_iterator = BatchIterator(val_dataset, batch_size=batch_size, vocab_created=False, vocab=None, target_col=None,\n",
    "                             word2index=train_iterator.word2index, sos_token='<SOS>', eos_token='<EOS>',\n",
    "                             unk_token='<UNK>', pad_token='<PAD>', min_word_count=3, max_vocab_size=None,\n",
    "                             max_seq_len=0.9, use_pretrained_vectors=False, glove_path='glove/',\n",
    "                             glove_name='glove.6B.100d.txt', weights_file_name='glove/weights.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check out if the batches look correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_seq shape:  torch.Size([32, 9])\n",
      "target shape:  torch.Size([32])\n",
      "x_lengths shape:  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batches in train_iterator:\n",
    "    # Unpack the dictionary of batches\n",
    "    input_seq, target, x_lengths = batches['input_seq'], batches['target'], batches['x_lengths']\n",
    "    print('input_seq shape: ', input_seq.size())\n",
    "    print('target shape: ', target.size())\n",
    "    print('x_lengths shape: ', x_lengths.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the maximum sequence length\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "for batches in train_iterator:\n",
    "    x_lengths = batches['x_lengths']\n",
    "    if max(x_lengths) > max_len:\n",
    "        max_len = int(max(x_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 59\n"
     ]
    }
   ],
   "source": [
    "print('Maximum sequence length: {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start implementing the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Implementation of the Multi-Head-Attention.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    dmodel: int\n",
    "        Dimensionality of the input embedding vector.\n",
    "    heads: int\n",
    "        Number of the self-attention operations to conduct in parallel. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dmodel, heads):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert dmodel % heads == 0, 'Embedding dimension is not divisible by number of heads'\n",
    "            \n",
    "        self.dmodel = dmodel\n",
    "        self.heads = heads\n",
    "        # Split dmodel (embedd dimension) into 'heads' number of chunks\n",
    "        # each chunk of size key_dim will be passed to different attention head\n",
    "        self.key_dim = dmodel // heads\n",
    "        \n",
    "        # keys, queries and values will be computed at once for all heads\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(self.dmodel, self.dmodel, bias=False),\n",
    "            nn.Linear(self.dmodel, self.dmodel, bias=False),\n",
    "            nn.Linear(self.dmodel, self.dmodel, bias=False)])\n",
    "        \n",
    "        self.concat = nn.Linear(self.dmodel, self.dmodel, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Perform Multi-Head-Attention.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: torch.Tensor\n",
    "            Batch of inputs - position encoded word embeddings ((batch_size, seq_length, embedding_dim)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Multi-Head-Attention output of a shape (batch_size, seq_len, dmodel)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.batch_size = inputs.size(0)\n",
    "        \n",
    "        assert inputs.size(2) == self.dmodel, 'Input sizes mismatch, dmodel={}, while embedd={}'\\\n",
    "            .format(self.dmodel, inputs.size(2))\n",
    "\n",
    "        # Inputs shape (batch_size, seq_length, embedding_dim)        \n",
    "        # Map input batch allong embedd dimension to query, key and value vectors with\n",
    "        # a shape of (batch_size, heads, seq_len, key_dim (dmodel // heads)) \n",
    "        # where 'heads' dimension corresponds o different attention head\n",
    "        query, key, value = [linear(x).view(self.batch_size, -1, self.heads, self.key_dim).transpose(1, 2)\\\n",
    "                             for linear, x in zip(self.linear, (inputs, inputs, inputs))]\n",
    "        \n",
    "        # Calculate the score (batch_size, heads, seq_len, seq_len)\n",
    "        # for all heads at once\n",
    "        score = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.key_dim)\n",
    "        \n",
    "        # Apply softmax to scores (batch_size, heads, seq_len, seq_len) \n",
    "        soft_score = F.softmax(score, dim = -1)\n",
    "        \n",
    "        # Multiply softmaxed score and value vector\n",
    "        # value input shape (batch_size, heads, seq_len, key_dim)\n",
    "        # out shape (batch_size, seq_len, dmodel (key_dim * heads))\n",
    "        out = torch.matmul(soft_score, value).transpose(1, 2).contiguous()\\\n",
    "            .view(self.batch_size, -1, self.heads * self.key_dim)\n",
    "        \n",
    "        # Concatenate and linearly transform heads to the lower dimensional space\n",
    "        # out shape (batch_size, seq_len, dmodel)\n",
    "        out = self.concat(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Implementation of the positional encoding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    max_len: int\n",
    "        The maximum expected sequence length.\n",
    "    dmodel: int\n",
    "        Dimensionality of the input embedding vector.\n",
    "    dropout: float\n",
    "        Probability of an element of the tensor to be zeroed.\n",
    "    padding_idx: int\n",
    "        Index of the padding token in the vocabulary and word embedding.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_len, dmodel, dropout, padding_idx):\n",
    "        \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "                \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create pos_encoding, positions and dimensions matrices\n",
    "        # with a shape of (max_len, dmodel)\n",
    "        self.pos_encoding = torch.zeros(max_len, dmodel)\n",
    "        positions = torch.repeat_interleave(torch.arange(float(max_len)).unsqueeze(1), dmodel, dim=1)\n",
    "        dimensions = torch.arange(float(dmodel)).repeat(max_len, 1)\n",
    "                                  \n",
    "        # Calculate the encodings trigonometric function argument (max_len, dmodel)\n",
    "        trig_fn_arg = positions / (torch.pow(10000, 2 * dimensions / dmodel))\n",
    "               \n",
    "        # Encode positions using sin function for even dimensions and\n",
    "        # cos function for odd dimensions\n",
    "        self.pos_encoding[:, 0::2] = torch.sin(trig_fn_arg[:, 0::2])\n",
    "        self.pos_encoding[:, 1::2] = torch.cos(trig_fn_arg[:, 1::2])\n",
    "        \n",
    "        # Set the padding positional encoding to zero tensor\n",
    "        if padding_idx:\n",
    "            self.pos_encoding[padding_idx] = 0.0\n",
    "        \n",
    "        # Add batch dimension\n",
    "        self.pos_encoding = self.pos_encoding.unsqueeze(0)\n",
    "        \n",
    "        \n",
    "    def forward(self, embedd):\n",
    "        \"\"\"Apply positional encoding.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        embedd: torch.Tensor\n",
    "            Batch of word embeddings ((batch_size, seq_length, dmodel = embedding_dim))\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Sum of word embeddings and positional embeddings (batch_size, seq_length, dmodel)\n",
    "        \"\"\"\n",
    "        \n",
    "        # embedd shape (batch_size, seq_length, embedding_dim)\n",
    "        # pos_encoding shape (1, max_len, dmodel = embedd_dim)\n",
    "        embedd = embedd + self.pos_encoding[:, :embedd.size(1), :]\n",
    "        embedd = self.dropout(embedd)\n",
    "        \n",
    "        # embedd shape (batch_size, seq_length, embedding_dim)\n",
    "        return embedd  \n",
    "    \n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    \"\"\"Implementation of label smoothing with the Kullback-Leibler divergence Loss.\n",
    "    \n",
    "    Example:\n",
    "    label_smoothing/(output_size-1) = 0.1\n",
    "    confidence = 1 - 0.1 = 0.9\n",
    "        \n",
    "    True labels      Smoothed one-hot labels\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |1|              [0.1000, 0.9000]\n",
    "        |1|    label     [0.1000, 0.9000]\n",
    "        |0|  smoothing   [0.9000, 0.1000]\n",
    "        |1|    ---->     [0.1000, 0.9000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |0|              [0.9000, 0.1000]\n",
    "        |1|              [0.1000, 0.9000]\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_size: int\n",
    "         The number of classes.\n",
    "    label_smoothing: float, optional (default=0)\n",
    "        The smoothing parameter. Takes the value in range [0,1].\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_size, label_smoothing=0):\n",
    "\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.confidence = 1 - self.label_smoothing\n",
    "        \n",
    "        assert label_smoothing >= 0.0 and label_smoothing <= 1.0, \\\n",
    "        'Label smoothing parameter takes values in the range [0, 1]'\n",
    "\n",
    "        self.criterion = nn.KLDivLoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"Smooth the target labels and calculate the Kullback-Leibler divergence loss.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        pred: torch.Tensor\n",
    "            Batch of log-probabilities (batch_size, output_size)\n",
    "        target: torch.Tensor\n",
    "            Batch of target labels (batch_size, seq_length)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The Kullback-Leibler divergence Loss.\n",
    "    \n",
    "        \"\"\"\n",
    "        # Create a Tensor of targets probabilities of a shape that equals 'pred' dimensions, filled all\n",
    "        # with label_smoothing/(output_size-1) value that will correspond to the wrong label probability.\n",
    "        one_hot_probs = torch.full(size=pred.size(), fill_value=self.label_smoothing/(self.output_size - 1))\n",
    "        \n",
    "        # Fill the tensor at positions that correspond to the true label from the target vector (0/1)\n",
    "        # with the modified value of maximum probability (confidence).\n",
    "        one_hot_probs.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "            \n",
    "        # KLDivLoss takes inputs (pred) that contain log-probs and targets given as probs (one_hot_probs).\n",
    "        return self.criterion(pred, one_hot_probs)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Implementation of single Transformer block.\n",
    "    \n",
    "    Transformer block structure:\n",
    "    x --> Multi-Head --> Layer normalization --> Pos-Wise FFNN --> Layer normalization --> y\n",
    "      |   Attention   |                       |                 |\n",
    "      |_______________|                       |_________________|\n",
    "     residual connection                      residual connection\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dmodel: int\n",
    "        Dimensionality of the input embedding vector.\n",
    "    ffnn_hidden_size: int\n",
    "        Position-Wise-Feed-Forward Neural Network hidden size.\n",
    "    heads: int\n",
    "        Number of the self-attention operations to conduct in parallel.\n",
    "    dropout: float\n",
    "        Probability of an element of the tensor to be zeroed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dmodel, ffnn_hidden_size, heads, dropout):\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(dmodel, heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(dmodel)\n",
    "        self.layer_norm2 = nn.LayerNorm(dmodel)\n",
    "        \n",
    "        self.ffnn = nn.Sequential(\n",
    "                nn.Linear(dmodel, ffnn_hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(ffnn_hidden_size, dmodel))\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward propagate through the Transformer block.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: torch.Tensor\n",
    "            Batch of embeddings.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output of the Transformer block (batch_size, seq_length, dmodel)\n",
    "        \"\"\"\n",
    "        # Inputs shape (batch_size, seq_length, embedding_dim = dmodel)\n",
    "        output = inputs + self.attention(inputs)            \n",
    "        output = self.layer_norm1(output)            \n",
    "        output = output + self.ffnn(output)            \n",
    "        output = self.layer_norm2(output)\n",
    "\n",
    "        # Output shape (batch_size, seq_length, dmodel)\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Implementation of the Transformer model for classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size: int\n",
    "        The size of the vocabulary.\n",
    "    dmodel: int\n",
    "        Dimensionality of the embedding vector.\n",
    "    max_len: int\n",
    "        The maximum expected sequence length.\n",
    "    padding_idx: int, optional (default=0)\n",
    "        Index of the padding token in the vocabulary and word embedding.\n",
    "    n_layers: int, optional (default=4)\n",
    "        Number of the stacked Transformer blocks.    \n",
    "    ffnn_hidden_size: int, optonal (default=dmodel * 4)\n",
    "        Position-Wise-Feed-Forward Neural Network hidden size.\n",
    "    heads: int, optional (default=8)\n",
    "        Number of the self-attention operations to conduct in parallel.\n",
    "    pooling: str, optional (default='max')\n",
    "        Specify the type of pooling to use. Available options: 'max' or 'avg'.\n",
    "    dropout: float, optional (default=0.2)\n",
    "        Probability of an element of the tensor to be zeroed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, dmodel, output_size, max_len, padding_idx=0, n_layers=4,\n",
    "                 ffnn_hidden_size=None, heads=8, pooling='max', dropout=0.2):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        if not ffnn_hidden_size:\n",
    "            ffnn_hidden_size = dmodel * 4\n",
    "            \n",
    "        assert pooling == 'max' or pooling == 'avg', 'Improper pooling type was passed.'\n",
    "        \n",
    "        self.pooling = pooling\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, dmodel)\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(max_len, dmodel, dropout, padding_idx)\n",
    "        \n",
    "        self.tnf_blocks = nn.ModuleList()\n",
    "        \n",
    "        for n in range(n_layers):\n",
    "            self.tnf_blocks.append(\n",
    "                TransformerBlock(dmodel, ffnn_hidden_size, heads, dropout))\n",
    "            \n",
    "        self.tnf_blocks = nn.Sequential(*self.tnf_blocks)\n",
    "            \n",
    "        self.linear = nn.Linear(dmodel, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, input_lengths):\n",
    "        \"\"\"Forward propagate through the Transformer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs: torch.Tensor\n",
    "            Batch of input sequences.\n",
    "        input_lengths: torch.LongTensor\n",
    "            Batch containing sequences lengths.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Logarithm of softmaxed class tensor.\n",
    "        \"\"\"\n",
    "        self.batch_size = inputs.size(0)\n",
    "        \n",
    "        # Input dimensions (batch_size, seq_length, dmodel)\n",
    "        output = self.embedding(inputs)\n",
    "        output = self.pos_encoding(output)\n",
    "        output = self.tnf_blocks(output)\n",
    "        # Output dimensions (batch_size, seq_length, dmodel)\n",
    "        \n",
    "        if self.pooling == 'max':\n",
    "            # Permute to the shape (batch_size, dmodel, seq_length)\n",
    "            # Apply max-pooling, output dimensions (batch_size, dmodel)\n",
    "            output = F.adaptive_max_pool1d(output.permute(0,2,1), (1,)).view(self.batch_size,-1)\n",
    "        else:\n",
    "            # Sum along the batch axis and divide by the corresponding lengths (FloatTensor)\n",
    "            # Output shape: (batch_size, dmodel)\n",
    "            output = torch.sum(output, dim=1) / input_lengths.view(-1,1).type(torch.FloatTensor) \n",
    "            \n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        \n",
    "    def add_loss_fn(self, loss_fn):\n",
    "        \"\"\"Add loss function to the model.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "\n",
    "    def add_optimizer(self, optimizer):\n",
    "        \"\"\"Add optimizer to the model.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        \n",
    "    def add_device(self, device=torch.device('cpu')):\n",
    "        \"\"\"Specify the device.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "    \n",
    "    \n",
    "    def train_model(self, train_iterator):\n",
    "        \"\"\"Perform single training epoch.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_iterator: BatchIterator\n",
    "            BatchIterator class object containing training batches.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        train_losses: list\n",
    "            List of the training average batch losses.\n",
    "        avg_loss: float\n",
    "            Average loss on the entire training set.\n",
    "        accuracy: float\n",
    "            Models accuracy on the entire training set.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        \n",
    "        train_losses = []\n",
    "        losses = []\n",
    "        losses_list = []\n",
    "        num_seq = 0\n",
    "        batch_correct = 0\n",
    "            \n",
    "        for i, batches in tqdm_notebook(enumerate(train_iterator, 1), total=len(train_iterator), desc='Training'):\n",
    "            input_seq, target, x_lengths = batches['input_seq'], batches['target'], batches['x_lengths']\n",
    "            \n",
    "            input_seq.to(self.device)\n",
    "            target.to(self.device)\n",
    "            x_lengths.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            pred = self.forward(input_seq, x_lengths)\n",
    "            loss = self.loss_fn(pred, target)\n",
    "            loss.backward()\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            losses_list.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            pred = torch.argmax(pred, 1)\n",
    "\n",
    "            if self.device.type == 'cpu':\n",
    "                batch_correct += (pred.cpu() == target.cpu()).sum().item()\n",
    "\n",
    "            else:\n",
    "                batch_correct += (pred == target).sum().item()\n",
    "\n",
    "            num_seq += len(input_seq)     \n",
    "    \n",
    "            if i % 100 == 0:\n",
    "                avg_train_loss = np.mean(losses)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                \n",
    "                accuracy = batch_correct / num_seq\n",
    "                \n",
    "                print('Iteration: {}. Average training loss: {:.4f}. Accuracy: {:.3f}'\\\n",
    "                      .format(i, avg_train_loss, accuracy))\n",
    "                \n",
    "                losses = []\n",
    "                \n",
    "            avg_loss = np.mean(losses_list)\n",
    "            accuracy = batch_correct / num_seq\n",
    "                              \n",
    "        return train_losses, avg_loss, accuracy\n",
    "    \n",
    "    \n",
    "    def evaluate_model(self, eval_iterator, conf_mtx=False):\n",
    "        \"\"\"Perform the one evaluation epoch.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        eval_iterator: BatchIterator\n",
    "            BatchIterator class object containing evaluation batches.\n",
    "        conf_mtx: boolean, optional (default=False)\n",
    "            Whether to print the confusion matrix at each epoch.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        eval_losses: list\n",
    "            List of the evaluation average batch losses.\n",
    "        avg_loss: float\n",
    "            Average loss on the entire evaluation set.\n",
    "        accuracy: float\n",
    "            Models accuracy on the entire evaluation set.\n",
    "        conf_matrix: list\n",
    "            Confusion matrix.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        eval_losses = []\n",
    "        losses = []\n",
    "        losses_list = []\n",
    "        num_seq = 0\n",
    "        batch_correct = 0\n",
    "        pred_total = torch.LongTensor()\n",
    "        target_total = torch.LongTensor()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, batches in tqdm_notebook(enumerate(eval_iterator, 1), total=len(eval_iterator), desc='Evaluation'):\n",
    "                input_seq, target, x_lengths = batches['input_seq'], batches['target'], batches['x_lengths']\n",
    "                \n",
    "                input_seq.to(self.device)\n",
    "                target.to(self.device)\n",
    "                x_lengths.to(self.device)\n",
    "\n",
    "                pred = self.forward(input_seq, x_lengths)\n",
    "                loss = self.loss_fn(pred, target)\n",
    "                losses.append(loss.data.cpu().numpy())\n",
    "                losses_list.append(loss.data.cpu().numpy())\n",
    "                \n",
    "                pred = torch.argmax(pred, 1)\n",
    "                                \n",
    "                if self.device.type == 'cpu':\n",
    "                    batch_correct += (pred.cpu() == target.cpu()).sum().item()\n",
    "                    \n",
    "                else:\n",
    "                    batch_correct += (pred == target).sum().item()\n",
    "                    \n",
    "                num_seq += len(input_seq)     \n",
    "                \n",
    "                pred_total = torch.cat([pred_total, pred], dim=0)\n",
    "                target_total = torch.cat([target_total, target], dim=0)\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    avg_batch_eval_loss = np.mean(losses)\n",
    "                    eval_losses.append(avg_batch_eval_loss)\n",
    "                    \n",
    "                    accuracy = batch_correct / num_seq\n",
    "                    \n",
    "                    print('Iteration: {}. Average evaluation loss: {:.4f}. Accuracy: {:.2f}'\\\n",
    "                          .format(i, avg_batch_eval_loss, accuracy))\n",
    "\n",
    "                    losses = []\n",
    "                    \n",
    "            avg_loss_list = []\n",
    "                    \n",
    "            avg_loss = np.mean(losses_list)\n",
    "            accuracy = batch_correct / num_seq\n",
    "            \n",
    "            conf_matrix = confusion_matrix(target_total.view(-1), pred_total.view(-1))\n",
    "        \n",
    "        if conf_mtx:\n",
    "            print('\\tConfusion matrix: ', conf_matrix)\n",
    "            \n",
    "        return eval_losses, avg_loss, accuracy, conf_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start epoch [1/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea99a29444f410186616d92b1981529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1505. Accuracy: 0.733\n",
      "Iteration: 200. Average training loss: 0.1446. Accuracy: 0.742\n",
      "Iteration: 300. Average training loss: 0.1452. Accuracy: 0.744\n",
      "Iteration: 400. Average training loss: 0.1405. Accuracy: 0.745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a898a46fa9b477da4a623eaa5b49be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1303. Accuracy: 0.76\n",
      "\n",
      "Epoch [1/30]: Train accuracy: 0.746. Train loss: 0.1444. Evaluation accuracy: 0.757. Evaluation loss: 0.1312\n",
      "\n",
      "Start epoch [2/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c048d8138dd2403aba0336c876373162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1366. Accuracy: 0.741\n",
      "Iteration: 200. Average training loss: 0.1355. Accuracy: 0.746\n",
      "Iteration: 300. Average training loss: 0.1376. Accuracy: 0.746\n",
      "Iteration: 400. Average training loss: 0.1297. Accuracy: 0.747\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e04978048a4d52b09228efd8978b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1222. Accuracy: 0.76\n",
      "\n",
      "Epoch [2/30]: Train accuracy: 0.746. Train loss: 0.1340. Evaluation accuracy: 0.757. Evaluation loss: 0.1227\n",
      "\n",
      "Start epoch [3/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90e5b63d3514e8e91ab0853ef9dbbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1292. Accuracy: 0.753\n",
      "Iteration: 200. Average training loss: 0.1257. Accuracy: 0.758\n",
      "Iteration: 300. Average training loss: 0.1282. Accuracy: 0.758\n",
      "Iteration: 400. Average training loss: 0.1225. Accuracy: 0.759\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab7988c967e435f873e837946d19d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1122. Accuracy: 0.79\n",
      "\n",
      "Epoch [3/30]: Train accuracy: 0.762. Train loss: 0.1256. Evaluation accuracy: 0.796. Evaluation loss: 0.1117\n",
      "\n",
      "Start epoch [4/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bf2b3c69ee4ad08777366d26b3d0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1200. Accuracy: 0.779\n",
      "Iteration: 200. Average training loss: 0.1201. Accuracy: 0.776\n",
      "Iteration: 300. Average training loss: 0.1248. Accuracy: 0.772\n",
      "Iteration: 400. Average training loss: 0.1159. Accuracy: 0.774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae491c72439420caeb8c2b06dd44ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1085. Accuracy: 0.80\n",
      "\n",
      "Epoch [4/30]: Train accuracy: 0.776. Train loss: 0.1197. Evaluation accuracy: 0.801. Evaluation loss: 0.1083\n",
      "\n",
      "Start epoch [5/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bcc072257043079621526edc9b3d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1140. Accuracy: 0.785\n",
      "Iteration: 200. Average training loss: 0.1143. Accuracy: 0.787\n",
      "Iteration: 300. Average training loss: 0.1186. Accuracy: 0.785\n",
      "Iteration: 400. Average training loss: 0.1096. Accuracy: 0.786\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4226abbd8a446a88eee39dd504cc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1045. Accuracy: 0.81\n",
      "\n",
      "Epoch [5/30]: Train accuracy: 0.787. Train loss: 0.1135. Evaluation accuracy: 0.809. Evaluation loss: 0.1045\n",
      "\n",
      "Start epoch [6/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a1e65a8a4045c38e3f716ca5c955c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1058. Accuracy: 0.808\n",
      "Iteration: 200. Average training loss: 0.1116. Accuracy: 0.801\n",
      "Iteration: 300. Average training loss: 0.1096. Accuracy: 0.801\n",
      "Iteration: 400. Average training loss: 0.1057. Accuracy: 0.800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae357735075a43a3b0455162d8e44627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.1019. Accuracy: 0.81\n",
      "\n",
      "Epoch [6/30]: Train accuracy: 0.801. Train loss: 0.1075. Evaluation accuracy: 0.812. Evaluation loss: 0.1019\n",
      "\n",
      "Start epoch [7/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeff92fa18c49e39970caccdd1ea1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1044. Accuracy: 0.808\n",
      "Iteration: 200. Average training loss: 0.1050. Accuracy: 0.806\n",
      "Iteration: 300. Average training loss: 0.1070. Accuracy: 0.804\n",
      "Iteration: 400. Average training loss: 0.0995. Accuracy: 0.808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ad4cc8b3c246b38002a7d546d98462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0992. Accuracy: 0.82\n",
      "\n",
      "Epoch [7/30]: Train accuracy: 0.808. Train loss: 0.1035. Evaluation accuracy: 0.820. Evaluation loss: 0.0993\n",
      "\n",
      "Start epoch [8/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edde676573045f3b92964bc333f353d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.1017. Accuracy: 0.812\n",
      "Iteration: 200. Average training loss: 0.1003. Accuracy: 0.814\n",
      "Iteration: 300. Average training loss: 0.1053. Accuracy: 0.814\n",
      "Iteration: 400. Average training loss: 0.0985. Accuracy: 0.816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706aaeb9d2774230bd99a6a7f6050ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0992. Accuracy: 0.82\n",
      "\n",
      "Epoch [8/30]: Train accuracy: 0.816. Train loss: 0.1016. Evaluation accuracy: 0.818. Evaluation loss: 0.0995\n",
      "\n",
      "Start epoch [9/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920dc8a0dfed406ca6f4e0103fb6eae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0961. Accuracy: 0.826\n",
      "Iteration: 200. Average training loss: 0.1001. Accuracy: 0.821\n",
      "Iteration: 300. Average training loss: 0.1035. Accuracy: 0.819\n",
      "Iteration: 400. Average training loss: 0.0966. Accuracy: 0.821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bcdc44ca694505b5a44f69c546659a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0965. Accuracy: 0.82\n",
      "\n",
      "Epoch [9/30]: Train accuracy: 0.822. Train loss: 0.0983. Evaluation accuracy: 0.820. Evaluation loss: 0.0966\n",
      "\n",
      "Start epoch [10/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97aa140369f49bd8296b75162d1c16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0938. Accuracy: 0.825\n",
      "Iteration: 200. Average training loss: 0.0979. Accuracy: 0.822\n",
      "Iteration: 300. Average training loss: 0.0984. Accuracy: 0.820\n",
      "Iteration: 400. Average training loss: 0.0900. Accuracy: 0.825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f858094a841b4984a036ec9b477b5822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0965. Accuracy: 0.82\n",
      "\n",
      "Epoch [10/30]: Train accuracy: 0.827. Train loss: 0.0940. Evaluation accuracy: 0.816. Evaluation loss: 0.0968\n",
      "\n",
      "Start epoch [11/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4ca073c81f455d9d44f7a849a02988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0888. Accuracy: 0.842\n",
      "Iteration: 200. Average training loss: 0.0918. Accuracy: 0.837\n",
      "Iteration: 300. Average training loss: 0.0941. Accuracy: 0.834\n",
      "Iteration: 400. Average training loss: 0.0900. Accuracy: 0.834\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fd6bc2c69743a7996e17019ce6be83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0960. Accuracy: 0.83\n",
      "\n",
      "Epoch [11/30]: Train accuracy: 0.837. Train loss: 0.0904. Evaluation accuracy: 0.825. Evaluation loss: 0.0966\n",
      "\n",
      "Start epoch [12/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8877d4cb9d8440d2a63574c3f42402ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0881. Accuracy: 0.844\n",
      "Iteration: 200. Average training loss: 0.0918. Accuracy: 0.840\n",
      "Iteration: 300. Average training loss: 0.0931. Accuracy: 0.839\n",
      "Iteration: 400. Average training loss: 0.0877. Accuracy: 0.839\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e056cbbdfbc647aead331291077082a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0945. Accuracy: 0.82\n",
      "\n",
      "Epoch [12/30]: Train accuracy: 0.841. Train loss: 0.0892. Evaluation accuracy: 0.824. Evaluation loss: 0.0947\n",
      "\n",
      "Start epoch [13/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa5033da2584a4c876d9cb7211832a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0900. Accuracy: 0.838\n",
      "Iteration: 200. Average training loss: 0.0878. Accuracy: 0.839\n",
      "Iteration: 300. Average training loss: 0.0936. Accuracy: 0.836\n",
      "Iteration: 400. Average training loss: 0.0861. Accuracy: 0.840\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2414d49b68764ea9a5ff41cd2a54d094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0943. Accuracy: 0.83\n",
      "\n",
      "Epoch [13/30]: Train accuracy: 0.842. Train loss: 0.0881. Evaluation accuracy: 0.831. Evaluation loss: 0.0944\n",
      "\n",
      "Start epoch [14/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d1372adb164fd2b49a59b801a454f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0852. Accuracy: 0.851\n",
      "Iteration: 200. Average training loss: 0.0859. Accuracy: 0.848\n",
      "Iteration: 300. Average training loss: 0.0907. Accuracy: 0.844\n",
      "Iteration: 400. Average training loss: 0.0829. Accuracy: 0.847\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0a47a6f07b4f8f859f125c644c0f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0948. Accuracy: 0.83\n",
      "\n",
      "Epoch [14/30]: Train accuracy: 0.848. Train loss: 0.0853. Evaluation accuracy: 0.827. Evaluation loss: 0.0944\n",
      "\n",
      "Start epoch [15/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c5ecdd665a4abe8a8f8215fcc4b8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0847. Accuracy: 0.846\n",
      "Iteration: 200. Average training loss: 0.0845. Accuracy: 0.847\n",
      "Iteration: 300. Average training loss: 0.0871. Accuracy: 0.848\n",
      "Iteration: 400. Average training loss: 0.0824. Accuracy: 0.849\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8513ce93671e46269ac8c0668a0131bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0936. Accuracy: 0.83\n",
      "\n",
      "Epoch [15/30]: Train accuracy: 0.850. Train loss: 0.0833. Evaluation accuracy: 0.830. Evaluation loss: 0.0943\n",
      "\n",
      "Start epoch [16/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f572ad576d774aa9bcf0dfe58ec8eca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0832. Accuracy: 0.852\n",
      "Iteration: 200. Average training loss: 0.0833. Accuracy: 0.853\n",
      "Iteration: 300. Average training loss: 0.0860. Accuracy: 0.852\n",
      "Iteration: 400. Average training loss: 0.0807. Accuracy: 0.854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865cd1001d8f4420a80f33ede1c0c2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0934. Accuracy: 0.83\n",
      "\n",
      "Epoch [16/30]: Train accuracy: 0.854. Train loss: 0.0827. Evaluation accuracy: 0.828. Evaluation loss: 0.0938\n",
      "\n",
      "Start epoch [17/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdc956be9854ec58341e3468e046668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0789. Accuracy: 0.860\n",
      "Iteration: 200. Average training loss: 0.0811. Accuracy: 0.858\n",
      "Iteration: 300. Average training loss: 0.0823. Accuracy: 0.858\n",
      "Iteration: 400. Average training loss: 0.0789. Accuracy: 0.858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fc6219f01641759f610b995f19cae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0932. Accuracy: 0.83\n",
      "\n",
      "Epoch [17/30]: Train accuracy: 0.859. Train loss: 0.0790. Evaluation accuracy: 0.829. Evaluation loss: 0.0936\n",
      "\n",
      "Start epoch [18/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a005626fd9334ed08cd69c184798ef73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0765. Accuracy: 0.867\n",
      "Iteration: 200. Average training loss: 0.0789. Accuracy: 0.863\n",
      "Iteration: 300. Average training loss: 0.0828. Accuracy: 0.860\n",
      "Iteration: 400. Average training loss: 0.0755. Accuracy: 0.863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fddb8d4d0b749e7902c2b2bd8d865e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0951. Accuracy: 0.82\n",
      "\n",
      "Epoch [18/30]: Train accuracy: 0.865. Train loss: 0.0773. Evaluation accuracy: 0.825. Evaluation loss: 0.0956\n",
      "\n",
      "Start epoch [19/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc1f17944e34a4abc330467b22c3f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0763. Accuracy: 0.871\n",
      "Iteration: 200. Average training loss: 0.0767. Accuracy: 0.870\n",
      "Iteration: 300. Average training loss: 0.0787. Accuracy: 0.869\n",
      "Iteration: 400. Average training loss: 0.0751. Accuracy: 0.869\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1717dca1d0f54108b03a078291cd0ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0943. Accuracy: 0.82\n",
      "\n",
      "Epoch [19/30]: Train accuracy: 0.869. Train loss: 0.0762. Evaluation accuracy: 0.825. Evaluation loss: 0.0944\n",
      "\n",
      "Start epoch [20/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1fe63264744840933b20803fe8d555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.0773. Accuracy: 0.872\n",
      "Iteration: 200. Average training loss: 0.0753. Accuracy: 0.867\n",
      "Iteration: 300. Average training loss: 0.0816. Accuracy: 0.865\n",
      "Iteration: 400. Average training loss: 0.0758. Accuracy: 0.866\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a278b1ef25a4f83bb3d0d2e430f625b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average evaluation loss: 0.0965. Accuracy: 0.82\n",
      "\n",
      "Epoch [20/30]: Train accuracy: 0.868. Train loss: 0.0762. Evaluation accuracy: 0.821. Evaluation loss: 0.0968\n",
      "\n",
      "Training stoped by EarlyStopping\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "vocab_size = len(train_iterator.word2index)\n",
    "dmodel = 64\n",
    "output_size = 2\n",
    "padding_idx = train_iterator.word2index['<PAD>']\n",
    "n_layers = 4\n",
    "ffnn_hidden_size = dmodel * 2\n",
    "heads = 8\n",
    "pooling = 'max'\n",
    "dropout = 0.5\n",
    "label_smoothing = 0.1\n",
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "\n",
    "# Check whether system supports CUDA\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "model = Transformer(vocab_size, dmodel, output_size, max_len, padding_idx, n_layers,\\\n",
    "                    ffnn_hidden_size, heads, pooling, dropout)\n",
    "\n",
    "# Move the model to GPU if possible\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "    \n",
    "# Add loss function    \n",
    "if label_smoothing:\n",
    "    loss_fn = LabelSmoothingLoss(output_size, label_smoothing)\n",
    "else:\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    \n",
    "model.add_loss_fn(loss_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.add_optimizer(optimizer)\n",
    "\n",
    "device = torch.device('cuda' if CUDA else 'cpu')\n",
    "\n",
    "model.add_device(device)\n",
    "\n",
    "# Create the parameters dictionary and instantiate the tensorboardX SummaryWriter\n",
    "params = {'batch_size': batch_size,\n",
    "          'dmodel': dmodel,\n",
    "          'n_layers': n_layers,\n",
    "          'ffnn_hidden_size': ffnn_hidden_size,\n",
    "          'heads': heads,\n",
    "          'pooling': pooling,\n",
    "          'dropout': dropout,\n",
    "          'label_smoothing': label_smoothing,\n",
    "          'learning_rate': learning_rate}\n",
    "\n",
    "train_writer = SummaryWriter(comment=f' Training, batch_size={batch_size}, dmodel={dmodel}, n_layers={n_layers},\\\n",
    "ffnn_hidden_size={ffnn_hidden_size}, heads={heads}, pooling={pooling}, dropout={dropout}, \\\n",
    "label_smoothing={label_smoothing}, learning_rate={learning_rate}'.format(**params))\n",
    "\n",
    "val_writer = SummaryWriter(comment=f' Validation, batch_size={batch_size}, dmodel={dmodel}, n_layers={n_layers},\\\n",
    "ffnn_hidden_size={ffnn_hidden_size}, heads={heads}, pooling={pooling}, dropout={dropout}, \\\n",
    "label_smoothing={label_smoothing}, learning_rate={learning_rate}'.format(**params))\n",
    "\n",
    "# Instantiate the EarlyStopping\n",
    "early_stop = EarlyStopping(wait_epochs=3)\n",
    "\n",
    "train_losses_list, train_avg_loss_list, train_accuracy_list = [], [], []\n",
    "eval_avg_loss_list, eval_accuracy_list, conf_matrix_list = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    try:\n",
    "        print('\\nStart epoch [{}/{}]'.format(epoch+1, epochs))\n",
    "\n",
    "        train_losses, train_avg_loss, train_accuracy = model.train_model(train_iterator)\n",
    "\n",
    "        train_losses_list.append(train_losses)\n",
    "        train_avg_loss_list.append(train_avg_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "        _, eval_avg_loss, eval_accuracy, conf_matrix = model.evaluate_model(val_iterator)\n",
    "\n",
    "        eval_avg_loss_list.append(eval_avg_loss)\n",
    "        eval_accuracy_list.append(eval_accuracy)\n",
    "        conf_matrix_list.append(conf_matrix)\n",
    "\n",
    "        print('\\nEpoch [{}/{}]: Train accuracy: {:.3f}. Train loss: {:.4f}. Evaluation accuracy: {:.3f}. Evaluation loss: {:.4f}'\\\n",
    "              .format(epoch+1, epochs, train_accuracy, train_avg_loss, eval_accuracy, eval_avg_loss))\n",
    "\n",
    "        train_writer.add_scalar('Training loss', train_avg_loss, epoch)\n",
    "        val_writer.add_scalar('Validation loss', eval_avg_loss, epoch)\n",
    "\n",
    "        if early_stop.stop(eval_avg_loss, model, delta=0.003):\n",
    "            break\n",
    "\n",
    "    finally:\n",
    "        train_writer.close()\n",
    "        val_writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will present validation learning curves of all training trials plotted by TensordBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div>\n",
    "<img src=\"assets/Trns_lc_TensorBoard.png\" width=\"800\"/>\n",
    "</div>\n",
    "<p><center>\n",
    "    Validation learning curves for the Transformer model (description below).\n",
    "</center></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tally of the three best hyperparameters sets is presented in the following table.\n",
    "\n",
    "| Curve description | Hyperparameters set | Loss | Accuracy |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| The lowest pink curve | batch_size=64, dmodel=64, n_layers=6, ffnn_hidden_size=128, heads=8, dropout=0.5, learning_rate=0.001 | 0.0692 | 0.882 |\n",
    "| The lowest blue curve | batch_size=32, dmodel=64, n_layers=6, ffnn_hidden_size=128, heads=4, dropout=0.5, learning_rate=0.001 | 0.0671 | 0.886 |\n",
    "| The lowest orange curve | batch_size=32, dmodel=64, n_layers=4, ffnn_hidden_size=128, heads=8, dropout=0.5, learning_rate=0.001 | 0.0693 | 0.880 |\n",
    "\n",
    "The orange curve listed in the above table corresponds to the current training process. The confusion matrix and generalization error will be also presented for the current training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(conf_matrix, fmt='d', annot=True, linewidths=1, square=True)\n",
    "ax.set_xlabel('Predictions', size=12)\n",
    "ax.set_ylabel('True labels', size=12) \n",
    "ax.set_title('Confusion Matrix', size=12); \n",
    "ax.xaxis.set_ticklabels(['True', 'False'])\n",
    "ax.yaxis.set_ticklabels(['True', 'False'])\n",
    "ax.set_ylim(2,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset. Use clean_review and label columns\n",
    "test_dataset = pd.read_csv('dataset/drugreview_feat_clean/test_feat_clean.csv',\n",
    "                          usecols=['clean_review', 'rating'])\n",
    "\n",
    "# Change columns order\n",
    "test_dataset['label'] = \n",
    "test_dataset = test_dataset[['clean_review', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>hear film till pop cable tv understand geena o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>know film show local tv kid remember watch see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>admire kiss mouth frankness Â– pubic hair cut m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>see cure kid love year late get hold copy acci...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>movie true fact see documentary day early movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_review  label\n",
       "0  hear film till pop cable tv understand geena o...      1\n",
       "1  know film show local tv kid remember watch see...      0\n",
       "2  admire kiss mouth frankness Â– pubic hair cut m...      0\n",
       "3  see cure kid love year late get hold copy acci...      1\n",
       "4  movie true fact see documentary day early movi...      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using as minimum count threashold: count = 3.00\n",
      "13480/39104 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 187\n",
      "Mapped words to indices\n",
      "Batches created\n"
     ]
    }
   ],
   "source": [
    "test_iterator = BatchIterator(test_dataset, batch_size=256, vocab_created=False, vocab=None, target_col=None,\n",
    "                              word2index=train_iterator.word2index, sos_token='<SOS>', eos_token='<EOS>',\n",
    "                              unk_token='<UNK>', pad_token='<PAD>', min_word_count=3, max_vocab_size=None,\n",
    "                              max_seq_len=0.9, use_pretrained_vectors=False, glove_path='glove/',\n",
    "                              glove_name='glove.6B.100d.txt', weights_file_name='glove/weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0f3958dd5c4866bc68bebcb35dbab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=20, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, test_avg_loss, test_accuracy, test_conf_matrix = model.evaluate_model(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.883. Test error: 0.070\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: {:.3f}. Test error: {:.3f}'.format(test_accuracy, test_avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(test_conf_matrix, fmt='d', annot=True, linewidths=1, square=True)\n",
    "ax.set_xlabel('Predictions', size=12)\n",
    "ax.set_ylabel('True labels', size=12) \n",
    "ax.set_title('Confusion Matrix', size=12); \n",
    "ax.xaxis.set_ticklabels(['True', 'False'])\n",
    "ax.yaxis.set_ticklabels(['True', 'False'])\n",
    "ax.set_ylim(2,0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

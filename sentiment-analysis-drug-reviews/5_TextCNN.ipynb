{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a TextCNN  model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import device\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score,roc_curve, roc_auc_score\n",
    "from batch_iterator import BatchIterator\n",
    "from early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS_RATING_THRESHOLD = 5\n",
    "POS_RATING_THRESHOLD = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset. Use clean_review and label columns\n",
    "train_dataset = pd.read_csv('drugreview/drugreview_feat_clean/train_feat_clean.csv', \n",
    "                            usecols=['clean_review', 'rating'])\n",
    "\n",
    "# Change columns order\n",
    "train_dataset['label'] = train_dataset.rating >= POS_RATING_THRESHOLD\n",
    "train_dataset = train_dataset[['clean_review', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>okay anxiety gotten worse past couple years po...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>reading possible effects scary medicine gave l...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>clonazepam effective controlling agitation pro...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>experienced effects considering anorexia nervo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>i&amp;#039;ve gianvi months skin clear didn&amp;#039;t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_review  label\n",
       "1   okay anxiety gotten worse past couple years po...   True\n",
       "6   reading possible effects scary medicine gave l...   True\n",
       "9   clonazepam effective controlling agitation pro...   True\n",
       "11  experienced effects considering anorexia nervo...   True\n",
       "12  i&#039;ve gianvi months skin clear didn&#039;t...   True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depict the first 5 rows of the training set\n",
    "train_dataset = train_dataset.dropna()\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset. Use clean_review and label columns\n",
    "val_dataset = pd.read_csv('drugreview/drugreview_feat_clean/val_feat_clean.csv',\n",
    "                          usecols=['clean_review', 'rating'])\n",
    "\n",
    "# Change columns order\n",
    "val_dataset['label'] = val_dataset.rating >= POS_RATING_THRESHOLD\n",
    "val_dataset = val_dataset[['clean_review', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4yrs having nexaplon implant mental physical h...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>l5 s1 lumbar herniated disc surgery weeks surg...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>far lot acne clear tea tree broke decided birt...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>insulin works fine trouble pen pain pen jammed...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>nexplanon option work iud painful insert pills...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        clean_review  label\n",
       "1  4yrs having nexaplon implant mental physical h...  False\n",
       "4  l5 s1 lumbar herniated disc surgery weeks surg...   True\n",
       "5  far lot acne clear tea tree broke decided birt...   True\n",
       "6  insulin works fine trouble pen pain pen jammed...   True\n",
       "7  nexplanon option work iud painful insert pills...   True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depict the first 5 rows of the validation set\n",
    "val_dataset = val_dataset.dropna()\n",
    "val_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using as minimum count threashold: count = 3.00\n",
      "14773/39267 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 48\n",
      "Mapped words to indices\n",
      "Start creating glove_word2vector dictionary\n",
      "Extracted 12312/14777 of pre-trained word vectors.\n",
      "2465 vectors initialized to random numbers\n",
      "Weights vectors saved into glove/weights_train.npy\n",
      "Batches created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_iterator = BatchIterator(train_dataset, batch_size=256, vocab_created=False, vocab=None, target_col=None,\n",
    "                               word2index=None, sos_token='<SOS>', eos_token='<EOS>', unk_token='<UNK>',\n",
    "                               pad_token='<PAD>', min_word_count=3, max_vocab_size=None, max_seq_len=0.7,\n",
    "                               use_pretrained_vectors=True, glove_path='glove/', glove_name='glove.6B.100d.txt',\n",
    "                               weights_file_name='glove/weights_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed vocabulary using as minimum count threashold: count = 3.00\n",
      "7720/19770 tokens has been retained\n",
      "Trimmed input strings vocabulary\n",
      "Trimmed input sequences lengths to the length of: 47\n",
      "Mapped words to indices\n",
      "Batches created\n"
     ]
    }
   ],
   "source": [
    "val_iterator = BatchIterator(val_dataset, batch_size=256, vocab_created=False, vocab=None, target_col=None,\n",
    "                             word2index=train_iterator.word2index, sos_token='<SOS>', eos_token='<EOS>',\n",
    "                             unk_token='<UNK>', pad_token='<PAD>', min_word_count=3, max_vocab_size=None,\n",
    "                             max_seq_len=0.7, use_pretrained_vectors=False, glove_path='glove/',\n",
    "                             glove_name='glove.6B.100d.txt', weights_file_name='glove/weights_val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to check out how batches that we created look like before we pass them into the model. For the record, the set of batches for input and output variables is returned as a dictionary, thus we will just look at the dictionary keys to find out how to extract particular variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_seq', 'target', 'x_lengths'])\n"
     ]
    }
   ],
   "source": [
    "for batches in train_iterator:\n",
    "    print(batches.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output batch has the dimensions: (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_seq shape:  torch.Size([256, 23])\n",
      "target shape:  torch.Size([256])\n",
      "x_lengths shape:  torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for batches in train_iterator:\n",
    "    # Unpack the dictionary of batches\n",
    "    input_seq, target, x_lengths = batches['input_seq'], batches['target'], batches['x_lengths']\n",
    "    print('input_seq shape: ', input_seq.size())\n",
    "    print('target shape: ', target.size())\n",
    "    print('x_lengths shape: ', x_lengths.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_seq shape:  torch.Size([256, 48])\n",
      "target shape:  torch.Size([256])\n",
      "x_lengths shape:  torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for batches in val_iterator:\n",
    "    # Unpack the dictionary of batches\n",
    "    input_seq, target, x_lengths = batches['input_seq'], batches['target'], batches['x_lengths']\n",
    "    print('input_seq shape: ', input_seq.size())\n",
    "    print('target shape: ', target.size())\n",
    "    print('x_lengths shape: ', x_lengths.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to build the TextCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, conv_config, weights_matrix, output_size, dropout=0.2):\n",
    "        \n",
    "        # Inherit everything from the nn.Module\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        # Initialize attributes\n",
    "        self.conv_config = conv_config\n",
    "        self.output_size = output_size\n",
    "        self.weights_matrix = weights_matrix\n",
    "        self.dropout_p = dropout\n",
    "        self.vocab_size, self.embedding_dim = self.weights_matrix.shape\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        # Load the weights to the embedding layer\n",
    "        self.embedding.load_state_dict({'weight': torch.from_numpy(weights_matrix)})\n",
    "        self.embedding.weight.requires_grad = False          \n",
    "            \n",
    "        self.convolutions = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv1d(self.embedding_dim, self.conv_config['num_channels'], kernel_size=kernel),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d((1,)))\\\n",
    "                         for kernel in self.conv_config['kernel_sizes']])    \n",
    "            \n",
    "        self.dropout = nn.Dropout(self.dropout_p)    \n",
    "        self.linear = nn.Linear(self.conv_config['num_channels'] * len(self.conv_config['kernel_sizes']),\\\n",
    "                                                                       self.output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        \n",
    "        # Embeddings shapes\n",
    "        # Input: (batch_size,  seq_length)\n",
    "        # Output: (batch_size, embedding_dim, seq_length)\n",
    "        emb_out = self.embedding(input_seq).permute(0,2,1)\n",
    "\n",
    "        # Conv1d -> Relu -> AdaptiveMaxPool1d\n",
    "        # Input: (batch_size, embedding_dim, seq_length)\n",
    "        # Output: (batch_size, num_channels)\n",
    "            \n",
    "        conv_out = [conv(emb_out).squeeze(2) for conv in self.convolutions]       \n",
    "\n",
    "        # Concatenate the list of convolving outputs from the previous step\n",
    "        concat_out = torch.cat(conv_out, dim=1)\n",
    "\n",
    "        concat_out = self.dropout(concat_out)\n",
    "        out = self.linear(concat_out)\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "    \n",
    "    \n",
    "    def add_loss_fn(self, loss_fn):\n",
    "\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "\n",
    "    def add_optimizer(self, optimizer):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        \n",
    "    def add_device(self, device=torch.device('cpu')):\n",
    "    \n",
    "        self.device = device\n",
    "    \n",
    "    \n",
    "    def train_model(self, train_iterator):\n",
    "       \n",
    "        self.train()\n",
    "        \n",
    "        losses_list = []\n",
    "        num_seq = 0\n",
    "        batch_correct = 0\n",
    "        train_losses = []\n",
    "        losses = []\n",
    "            \n",
    "        for i, batches in tqdm_notebook(enumerate(train_iterator, 1), total=len(train_iterator), desc='Training'):\n",
    "            input_seq, target, x_lengths = batches['input_seq'], batches['target'], batches['x_lengths']\n",
    "            \n",
    "            input_seq.to(self.device)\n",
    "            target.to(self.device)\n",
    "            x_lengths.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            pred = self.forward(input_seq)\n",
    "            loss = self.loss_fn(pred, target)\n",
    "            loss.backward()\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            losses_list.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            pred = torch.argmax(pred, 1)\n",
    "\n",
    "            if self.device.type == 'cpu':\n",
    "                batch_correct += (pred.cpu() == target.cpu()).sum().item()\n",
    "\n",
    "            else:\n",
    "                batch_correct += (pred == target).sum().item()\n",
    "\n",
    "            num_seq += len(input_seq)     \n",
    "    \n",
    "            if i % 100 == 0:\n",
    "                avg_train_loss = np.mean(losses)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                \n",
    "                accuracy = batch_correct / num_seq\n",
    "                \n",
    "                print('Iteration: {}. Average training loss: {:.4f}. Accuracy: {:.3f}'\\\n",
    "                      .format(i, avg_train_loss, accuracy))\n",
    "                \n",
    "                losses = []\n",
    "                \n",
    "            avg_loss = np.mean(losses_list)\n",
    "            accuracy = batch_correct / num_seq\n",
    "                              \n",
    "        return train_losses, avg_loss, accuracy\n",
    "    \n",
    "    \n",
    "    def evaluate_model(self, eval_iterator, conf_mtx=False):\n",
    "       \n",
    "        self.eval()\n",
    "        \n",
    "        eval_losses = []\n",
    "        losses = []\n",
    "        losses_list = []\n",
    "        num_seq = 0\n",
    "        batch_correct = 0\n",
    "        pred_total = torch.LongTensor()\n",
    "        target_total = torch.LongTensor()\n",
    "        all_y_hat = torch.FloatTensor()\n",
    "        all_y_true = torch.LongTensor()\n",
    "        all_y_pred = torch.LongTensor()\n",
    " \n",
    "        with torch.no_grad():\n",
    "            for i, batches in tqdm_notebook(enumerate(eval_iterator, 1), total=len(eval_iterator), desc='Evaluation'):\n",
    "                input_seq, target, x_lengths = batches['input_seq'], batches['target'], batches['x_lengths']\n",
    "                \n",
    "                input_seq.to(self.device)\n",
    "                target.to(self.device)\n",
    "                x_lengths.to(self.device)\n",
    "\n",
    "                pred = self.forward(input_seq)\n",
    "                loss = self.loss_fn(pred, target)\n",
    "                losses.append(loss.data.cpu().numpy())\n",
    "                losses_list.append(loss.data.cpu().numpy())\n",
    "                \n",
    "                #y_hat = pred.view(pred.shape[1])\n",
    "                #y_pred = y_hat.clone()\n",
    "                #y_pred = torch.tensor([1 if p > 0.5 else 0 for p in y_hat])\n",
    "                y_pred = np.exp(pred[:,1])\n",
    "                y_pred  = np.multiply((y_pred>0.5),1.0)\n",
    "                all_y_pred =  torch.cat((all_y_pred, y_pred.to('cpu').long()),dim=0)\n",
    "                all_y_hat =  torch.cat((all_y_hat, pred.to('cpu').float()),dim=0)\n",
    "                \n",
    "                \n",
    "                pred = torch.argmax(pred, 1)\n",
    "                if self.device.type == 'cpu':\n",
    "                    batch_correct += (pred.cpu() == target.cpu()).sum().item()\n",
    "                    \n",
    "                else:\n",
    "                    batch_correct += (pred == target).sum().item()\n",
    "                    \n",
    "                num_seq += len(input_seq)     \n",
    "                \n",
    "                pred_total = torch.cat([pred_total, pred], dim=0)\n",
    "                target_total = torch.cat([target_total, target], dim=0)\n",
    "\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    avg_batch_eval_loss = np.mean(losses)\n",
    "                    eval_losses.append(avg_batch_eval_loss)\n",
    "                    \n",
    "                    accuracy = batch_correct / num_seq\n",
    "                    \n",
    "                    print('Iteration: {}. Average evaluation loss: {:.4f}. Accuracy: {:.2f}'\\\n",
    "                          .format(i, avg_batch_eval_loss, accuracy))\n",
    "\n",
    "                    losses = []\n",
    "                    \n",
    "            avg_loss_list = []\n",
    "                    \n",
    "            avg_loss = np.mean(losses_list)\n",
    "            accuracy = batch_correct / num_seq\n",
    "   \n",
    "            conf_matrix = confusion_matrix(target_total.view(-1), pred_total.view(-1))\n",
    "            \n",
    "            roc = roc_auc_score(target_total.view(-1),all_y_hat[:,1])\n",
    "            fone_score = f1_score(target_total.view(-1),pred_total.view(-1))\n",
    "            fpr, tpr, threshold1 = roc_curve(target_total.view(-1), all_y_hat[:,1])\n",
    "\n",
    "        if conf_mtx:\n",
    "            print('\\tConfusion matrix: ', conf_matrix)\n",
    "            \n",
    "        return eval_losses, avg_loss, accuracy, conf_matrix,fone_score,fpr,tpr,roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start epoch [1/50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66a733c98dc4b53abfc6bb423d06ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.4553. Accuracy: 0.822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd03b46c03bd4a4a864d8239f429ba99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/50]: Train accuracy: 0.825. Train loss: 0.4347. Evaluation accuracy: 0.830. Evaluation loss: 0.3907\n",
      "\n",
      "Start epoch [2/50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d493e81597ce4bc0856ad1a7e3ecc6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.3898. Accuracy: 0.834\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b3695145b949b0a064cee798a6122e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/50]: Train accuracy: 0.837. Train loss: 0.3835. Evaluation accuracy: 0.842. Evaluation loss: 0.3669\n",
      "\n",
      "Start epoch [3/50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c7db72698845ea855eaaa916cf61cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.3708. Accuracy: 0.842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe594e1ae1a240f8a2c812c545df3c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/50]: Train accuracy: 0.845. Train loss: 0.3656. Evaluation accuracy: 0.846. Evaluation loss: 0.3564\n",
      "\n",
      "Start epoch [4/50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713e1fd5c594425f87428446dca25b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.3564. Accuracy: 0.848\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03b5bdf2d2249bd8a38c6a98cc2dcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/50]: Train accuracy: 0.850. Train loss: 0.3520. Evaluation accuracy: 0.852. Evaluation loss: 0.3495\n",
      "\n",
      "Start epoch [5/50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a329155dc442f1aa510a6bb4e0213c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.3448. Accuracy: 0.852\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f7752894e744b5bbdcfe7adf5b7c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/50]: Train accuracy: 0.854. Train loss: 0.3414. Evaluation accuracy: 0.853. Evaluation loss: 0.3460\n",
      "\n",
      "Start epoch [6/50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd0cc35c91340d4bb080253a0142424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.3381. Accuracy: 0.854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477a14a671834ce5958fa74f8c810462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/50]: Train accuracy: 0.856. Train loss: 0.3350. Evaluation accuracy: 0.856. Evaluation loss: 0.3426\n",
      "\n",
      "Start epoch [7/50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02b179f08c74b329a8cf2f04090b065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.3296. Accuracy: 0.857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbc665aac754ba0a0a945cdd8e1dfa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/50]: Train accuracy: 0.861. Train loss: 0.3259. Evaluation accuracy: 0.859. Evaluation loss: 0.3402\n",
      "\n",
      "Start epoch [8/50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a3f3695bac464d82fa99d225ed1db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Average training loss: 0.3236. Accuracy: 0.860\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f69a8696d6d4d8980f77e6156616e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/50]: Train accuracy: 0.863. Train loss: 0.3198. Evaluation accuracy: 0.860. Evaluation loss: 0.3391\n",
      "\n",
      "Start epoch [9/50]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f7f171bd784cceb1ebfb4f28f4e6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize parameters\n",
    "conv_config = {'num_channels': 50, 'kernel_sizes': [1,2]}\n",
    "output_size = 2\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "dropout = 0.2\n",
    "\n",
    "# Load the weights matrix\n",
    "weights = np.load('glove/weights_train.npy')\n",
    "\n",
    "# Check whether system supports CUDA\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "model = TextCNN(conv_config, weights, output_size, dropout)\n",
    "\n",
    "# Move the model to GPU if possible\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "model.add_loss_fn(nn.NLLLoss())\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.add_optimizer(optimizer)\n",
    "\n",
    "device = torch.device('cuda' if CUDA else 'cpu')\n",
    "\n",
    "model.add_device(device)\n",
    "\n",
    "# Instantiate the EarlyStopping\n",
    "early_stop = EarlyStopping(wait_epochs=3)\n",
    "\n",
    "train_losses_list, train_avg_loss_list, train_accuracy_list = [], [], []\n",
    "eval_avg_loss_list, eval_accuracy_list, conf_matrix_list,f1_score_list,roc_list = [], [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print('\\nStart epoch [{}/{}]'.format(epoch+1, epochs))\n",
    "    \n",
    "    train_losses, train_avg_loss, train_accuracy = model.train_model(train_iterator)\n",
    "    \n",
    "    train_losses_list.append(train_losses)\n",
    "    train_avg_loss_list.append(train_avg_loss)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    \n",
    "    _, eval_avg_loss, eval_accuracy, conf_matrix,fone_score,fpr,tpr,roc = model.evaluate_model(val_iterator)\n",
    "    \n",
    "    eval_avg_loss_list.append(eval_avg_loss)\n",
    "    eval_accuracy_list.append(eval_accuracy)\n",
    "    conf_matrix_list.append(conf_matrix)\n",
    "    f1_score_list.append(fone_score)\n",
    "    roc_list.append(roc)\n",
    "    \n",
    "    print('\\nEpoch [{}/{}]: Train accuracy: {:.3f}. Train loss: {:.4f}. Evaluation accuracy: {:.3f}. Evaluation loss: {:.4f}'\\\n",
    "          .format(epoch+1, epochs, train_accuracy, train_avg_loss, eval_accuracy, eval_avg_loss))\n",
    "    \n",
    "    if early_stop.stop(eval_avg_loss, model, delta=0.003):\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the dataset initial loss\n",
    "train_avg_loss_list.insert(0, train_losses_list[0][0])\n",
    "eval_avg_loss_list.insert(0, train_losses_list[0][0])\n",
    "print(\"f1_score:\",np.mean(f1_score_list))\n",
    "print(\"roc:\",np.mean(roc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and the validation learning curve\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_avg_loss_list, label='Training loss')\n",
    "plt.plot(eval_avg_loss_list, label='Evaluation loss')\n",
    "plt.xlabel('Epoch', size=12)\n",
    "plt.ylabel('Loss', size=12)\n",
    "plt.title('TextCNN model learning curves')\n",
    "plt.xticks(ticks=range(0,49,2))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(conf_matrix, fmt='d', annot=True, linewidths=1, square=True)\n",
    "ax.set_xlabel('Predictions', size=12)\n",
    "ax.set_ylabel('True labels', size=12) \n",
    "ax.set_title('Confusion Matrix', size=12); \n",
    "ax.xaxis.set_ticklabels(['True', 'False'])\n",
    "ax.yaxis.set_ticklabels(['True', 'False'])\n",
    "ax.set_ylim(2,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset. Use clean_review and label columns\n",
    "test_dataset = pd.read_csv('drugreview/drugreview_feat_clean/test_feat_clean.csv',\n",
    "                          usecols=['clean_review', 'rating'])\n",
    "\n",
    "# Change columns order\n",
    "test_dataset['label'] = test_dataset.rating >= POS_RATING_THRESHOLD\n",
    "test_dataset = test_dataset[['clean_review', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.dropna()\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterator = BatchIterator(test_dataset, batch_size=256, vocab_created=False, vocab=None, target_col=None,\n",
    "                              word2index=train_iterator.word2index, sos_token='<SOS>', eos_token='<EOS>',\n",
    "                              unk_token='<UNK>', pad_token='<PAD>', min_word_count=3, max_vocab_size=None,\n",
    "                              max_seq_len=0.9, use_pretrained_vectors=True, glove_path='glove/',\n",
    "                              glove_name='glove.6B.100d.txt', weights_file_name='glove/weights_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_avg_loss, test_accuracy, test_conf_matrix,test_f1_score,fpr,tpr,roc = model.evaluate_model(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy: {:.3f}. Test error: {:.3f}. Test f1_score{:.3f} Test roc{:.3f}'.format(test_accuracy, test_avg_loss,test_f1_score,roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(test_conf_matrix, fmt='d', annot=True, linewidths=1, square=True)\n",
    "ax.set_xlabel('Predictions', size=12)\n",
    "ax.set_ylabel('True labels', size=12) \n",
    "ax.set_title('Confusion Matrix', size=12); \n",
    "ax.xaxis.set_ticklabels(['True', 'False'])\n",
    "ax.yaxis.set_ticklabels(['True', 'False'])\n",
    "ax.set_ylim(2,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization accuracy of the TextCNN model is 0.839. As we can see in the above plot of the confusion matrix the number of False negative predictions (760) is greater than the amount of False positive predictions (7601) which is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, figsize=(5,5))\n",
    "plt.title('Receiver Operating Characteristic - TextCNN')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.7",
   "language": "python",
   "name": "py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
